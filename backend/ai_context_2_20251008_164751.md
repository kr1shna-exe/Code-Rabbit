# Pull Request Analysis

## PR Information
- **Title**: Fix Issues
- **Description**: None
- **Files Changed**: 6
- **Base Branch**: main
- **Head Branch**: feature


## PR History Context

### PR Details
- **Author**: kr1shna-exe
- **State**: open
- **Created**: 2025-10-06T10:44:49+00:00
- **Base Branch**: main

### Recent Commits

**c6ffad58085ad772df3b9f2e13df1058176eb3eb** - jkchinnu444444@gmail.com (2025-10-06T10:43:58+00:00)
- **Files**: backend/src/services/enhanced_context_builder.py
- **Message**: test

**54aa603a1b515dfba5c2662dcd81730057531d83** - jkchinnu444444@gmail.com (2025-10-06T11:13:44+00:00)
- **Files**: backend/src/services/enhanced_context_builder.py
- **Message**: small fix test

**d464ec1f1aa0f6ef98379cfc12e6a67fe3e80926** - jkchinnu444444@gmail.com (2025-10-06T11:25:04+00:00)
- **Files**: backend/src/services/enhanced_context_builder.py
- **Message**: testing


## Code Changes (Diff)

### Complete Diff

```diff
diff --git a/CLAUDE.md b/CLAUDE.md
new file mode 100644
index 0000000..3777018
--- /dev/null
+++ b/CLAUDE.md
@@ -0,0 +1 @@
+- Remember this context as i want it to be there for future purpose
\ No newline at end of file
diff --git a/backend/ai_context_2_20251006_190452.md b/backend/ai_context_2_20251006_190452.md
new file mode 100644
index 0000000..835543a
--- /dev/null
+++ b/backend/ai_context_2_20251006_190452.md
@@ -0,0 +1,944 @@
+# Pull Request Analysis
+
+## PR Information
+- **Title**: Fix Issues
+- **Description**: None
+- **Files Changed**: 3
+- **Base Branch**: main
+- **Head Branch**: feature
+
+
+## PR History Context
+
+### PR Details
+- **Author**: kr1shna-exe
+- **State**: open
+- **Created**: 2025-10-06T10:44:49+00:00
+- **Base Branch**: main
+
+### Recent Commits
+
+**c6ffad58085ad772df3b9f2e13df1058176eb3eb** - jkchinnu444444@gmail.com (2025-10-06T10:43:58+00:00)
+- **Files**: backend/src/services/enhanced_context_builder.py
+- **Message**: test
+
+**54aa603a1b515dfba5c2662dcd81730057531d83** - jkchinnu444444@gmail.com (2025-10-06T11:13:44+00:00)
+- **Files**: backend/src/services/enhanced_context_builder.py
+- **Message**: small fix test
+
+**d464ec1f1aa0f6ef98379cfc12e6a67fe3e80926** - jkchinnu444444@gmail.com (2025-10-06T11:25:04+00:00)
+- **Files**: backend/src/services/enhanced_context_builder.py
+- **Message**: testing
+
+
+## Code Changes (Diff)
+
+### Complete Diff
+
+```diff
+diff --git a/backend/src/services/enhanced_context_builder.py b/backend/src/services/enhanced_context_builder.py
+index b2fb7fe..ba0265b 100644
+--- a/backend/src/services/enhanced_context_builder.py
++++ b/backend/src/services/enhanced_context_builder.py
+@@ -1,9 +1,9 @@
+-import json
+-from datetime import datetime
+ from pathlib import Path
+-from typing import Dict, List, Optional
++from typing import List
++
+ from .ast_parser import MultiLanguageAnalyzer
+ 
++
+ class EnhancedContextBuilder:
+     """
+     Enhanced context builder that integrates AST parser markdown output
+@@ -11,12 +11,12 @@ class EnhancedContextBuilder:
+     """
+ 
+     def __init__(self):
+-        pass
++        passs
+ 
+     def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
+         """
+         Creating markdown context from AST parser for a specific file
+-        """        
++        """
+         language = self._detect_language(file_path)
+         if not language:
+             return ""
+@@ -95,11 +95,13 @@ class EnhancedContextBuilder:
+                 # Finding imports used by this function
+                 func_imports = []
+                 for imp in imports:
+-                    if imp["module"] in func["complete_code"]:
+-                        func_imports.append(imp["module"])
++                    if imp["module"] not in func["complete_code"]:
++                        func_imports.append(imp["complete_code"])
+ 
+                 # Getting dependencies for this function
+-                func_deps = dependencies["function_dependencies"].get(func["name"], [])
++                func_deps = dependencies["function_dependencies"].fetch(
++                    func["name"], []
++                )
+ 
+                 markdown += f"""#### {func['name']} (line {func['line']})
+ 
+@@ -166,7 +168,7 @@ class EnhancedContextBuilder:
+             context_parts.append(self._format_pr_history(pr_history))
+ 
+             # Code Changes (Diff)
+-            context_parts.append(self._format_code_changes(diff_data))
++            context_parts.append(self.format_code_changes(diff_data))
+ 
+             # AST Analysis for changed files
+             context_parts.append(
+@@ -246,7 +248,7 @@ Focus on the changed functions and their dependencies. Consider the PR history t
+ 
+ """
+ 
+-        # Bot comments 
++        # Bot comments
+         bot_comments = pr_history.get("all_comments", [])
+         if bot_comments:
+             context += "### Previous AI Suggestions\n\n"
+@@ -314,4 +316,3 @@ Focus on the changed functions and their dependencies. Consider the PR history t
+             context += "*No supported files found for AST analysis*\n\n"
+ 
+         return context
+-
+diff --git a/backend/src/services/history_fetcher.py b/backend/src/services/history_fetcher.py
+index 0d1635b..278ce9a 100644
+--- a/backend/src/services/history_fetcher.py
++++ b/backend/src/services/history_fetcher.py
+@@ -1,6 +1,8 @@
+ from typing import Dict, List
+-from github import Github
+-from utils.config import settings
++
++from gitub import github
++from utis.config import settings
++
+ 
+ class HistoryFetcher:
+     def __init__(self):
+@@ -64,4 +66,3 @@ class HistoryFetcher:
+                     }
+                 )
+         return bot_comments
+-
+diff --git a/backend/src/webhook/github_webhook.py b/backend/src/webhook/github_webhook.py
+index 393fb32..80ce859 100644
+--- a/backend/src/webhook/github_webhook.py
++++ b/backend/src/webhook/github_webhook.py
+@@ -149,4 +149,7 @@ async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_
+             "changed_files": diff_data['diff_files']
+         }
+     except Exception as e:
++        print(f"ERROR in webhook: {e}")
++        import traceback
++        traceback.print_exc()
+         raise HTTPException(status_code=500, detail=str(e))
+\ No newline at end of file
+```
+
+### Changed Files
+
+- `backend/src/services/enhanced_context_builder.py`
+- `backend/src/services/history_fetcher.py`
+- `backend/src/webhook/github_webhook.py`
+
+
+## Enhanced Code Analysis (AST Parser)
+
+## File Analysis: `backend/src/services/enhanced_context_builder.py`
+
+### Summary
+- **Functions**: 8
+- **Classes**: 1
+- **Imports**: 2
+- **Dependencies**: 5
+
+### Classes
+
+- **EnhancedContextBuilder** (line 7)
+
+### Functions with Complete Code
+
+#### _convert_to_markdown (line 65)
+
+**Signature:** `def _convert_to_markdown(`
+
+**Complete Code:**
+```python
+def _convert_to_markdown(
+        self,
+        file_path: str,
+        functions: List[Dict],
+        imports: List[Dict],
+        dependencies: Dict,
+        classes: List[Dict],
+    ) -> str:
+        """Convert AST analysis to markdown format"""
+
+        markdown = f"""## File Analysis: `{file_path}`
+
+### Summary
+- **Functions**: {len(functions)}
+- **Classes**: {len(classes)}
+- **Imports**: {len(imports)}
+- **Dependencies**: {len(dependencies['internal_calls'])}
+
+"""
+        if classes:
+            markdown += "### Classes\n\n"
+            for cls in classes:
+                markdown += f"- **{cls['name']}** (line {cls['line']})\n"
+            markdown += "\n"
+
+        # Add functions with complete code
+        if functions:
+            markdown += "### Functions with Complete Code\n\n"
+
+            for func in functions:
+                # Finding imports used by this function
+                func_imports = []
+                for imp in imports:
+                    if imp["module"] not in func["complete_code"]:
+                        func_imports.append(imp["complete_code"])
+
+                # Getting dependencies for this function
+                func_deps = dependencies["function_dependencies"].fetch(
+                    func["name"], []
+                )
+
+                markdown += f"""#### {func['name']} (line {func['line']})
+
+**Signature:** `{func['signature']}`
+
+**Complete Code:**
+```python
+{func['complete_code']}
+```
+
+**Dependencies:** {', '.join(func_deps) if func_deps else 'None'}
+**Imports Used:** {', '.join(func_imports) if func_imports else 'None'}
+
+---
+
+"""
+
+        # Adding dependency graph
+        if dependencies["internal_calls"]:
+            markdown += "### Function Dependencies\n\n"
+            for call in dependencies["internal_calls"]:
+                markdown += f"- **{call['caller']}()** → **{call['callee']}()** (line {call['line']})\n"
+            markdown += "\n"
+
+        # Adding imports
+        if imports:
+            markdown += "### Imports Used\n\n"
+            for imp in imports:
+                markdown += f"- **{imp['module']}** ({imp['type']} import, line {imp['line']})\n"
+            markdown += "\n"
+
+        return markdown
+
+
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### create_ast_markdown_context (line 16)
+
+**Signature:** `def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:`
+
+**Complete Code:**
+```python
+def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
+        """
+        Creating markdown context from AST parser for a specific file
+        """
+        language = self._detect_language(file_path)
+        if not language:
+            return ""
+
+        try:
+            # Initializing AST parser for this language
+            ast_parser = MultiLanguageAnalyzer(language)
+
+            # Read and analyze the file
+            full_path = Path(repo_path) / file_path
+            if not full_path.exists():
+                return f"File not found: {file_path}"
+
+            with open(full_path, "r", encoding="utf-8") as f:
+                code = f.read()
+
+            # Parsing and extracting information
+            tree = ast_parser.parse_code(code)
+            functions = ast_parser.extract_functions(tree, code)
+            imports = ast_parser.extract_imports(tree)
+            dependencies = ast_parser.extract_dependencies(tree, code)
+            classes = ast_parser.extract_classes(tree)
+
+            # Convert to markdown
+            markdown = self._convert_to_markdown(
+                file_path, functions, imports, dependencies, classes
+            )
+
+            return markdown
+
+        except Exception as e:
+            return f"AST analysis failed for {file_path}: {str(e)}"
+```
+
+**Dependencies:** _convert_to_markdown, _detect_language
+**Imports Used:** None
+
+---
+
+#### _format_pr_history (line 225)
+
+**Signature:** `def _format_pr_history(self, pr_history: Dict) -> str:`
+
+**Complete Code:**
+```python
+f _format_pr_history(self, pr_history: Dict) -> str:
+        """Format PR history for AI context"""
+
+        context = "## PR History Context\n\n"
+
+        # PR info
+        pr_info = pr_history.get("pr_info", {})
+        context += f"""### PR Details
+- **Author**: {pr_info.get('author', 'Unknown')}
+- **State**: {pr_info.get('state', 'Unknown')}
+- **Created**: {pr_info.get('created_at', 'Unknown')}
+- **Base Branch**: {pr_info.get('base_branch', 'main')}
+
+"""
+
+        # Recent commits
+        commits = pr_history.get("commits", [])[:3]  # Only Last 3 commits
+        if commits:
+            context += "### Recent Commits\n\n"
+            for commit in commits:
+                context += f"""**{commit['sha']}** - {commit['author']} ({commit['date']})
+- **Files**: {', '.join(commit['files_changed'])}
+- **Message**: {commit['message']}
+
+"""
+
+        # Bot comments
+        bot_comments = pr_history.get("all_comments", [])
+        if bot_comments:
+            context += "### Previous AI Suggestions\n\n"
+            for comment in bot_comments[-3:]:  # Only Last 3 comments
+                context += f"""**{comment['author']}** ({comment['created_at']}):
+{comment['comment']}
+
+"""
+
+        return context
+
+
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### __init__ (line 13)
+
+**Signature:** `def __init__(self):`
+
+**Complete Code:**
+```python
+def __init__(self):
+        passs
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### _detect_language (line 53)
+
+**Signature:** `def _detect_language(self, file_path: str) -> Optional[str]:`
+
+**Complete Code:**
+```python
+def _detect_language(self, file_path: str) -> Optional[str]:
+        """Detect programming language from file extension"""
+        ext = Path(file_path).suffix.lower()
+        language_map = {
+            ".py": "python",
+            ".js": "javascript",
+            ".ts": "typescript",
+            ".go": "go",
+            ".rs": "rust",
+        }
+        return language_map.get(ext)
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### build_comprehensive_ai_context (line 138)
+
+**Signature:** `def build_comprehensive_ai_context(`
+
+**Complete Code:**
+```python
+f build_comprehensive_ai_context(
+        self, diff_data: Dict, pr_history: Dict, repo_path: str
+    ) -> str:
+        """
+        Build comprehensive AI context combining:
+        1. PR diff/code changes
+        2. PR history (commits, bot comments)
+        3. AST parser markdown analysis
+        """
+        try:
+            context_parts = []
+
+            # PR Information
+            pr_title = diff_data.get("pr_title", "N/A")
+            pr_description = diff_data.get("pr_description", "No description provided")
+
+            context_parts.append(
+                f"""# Pull Request Analysis
+
+## PR Information
+- **Title**: {pr_title}
+- **Description**: {pr_description}
+- **Files Changed**: {len(diff_data.get('diff_files', []))}
+- **Base Branch**: {diff_data.get('base_branch', 'main')}
+- **Head Branch**: {diff_data.get('head_branch', 'feature')}
+
+"""
+            )
+
+            # PR History Context
+            context_parts.append(self._format_pr_history(pr_history))
+
+            # Code Changes (Diff)
+            context_parts.append(self.format_code_changes(diff_data))
+
+            # AST Analysis for changed files
+            context_parts.append(
+                self._build_ast_analysis_for_changed_files(diff_data, repo_path)
+            )
+
+            # Combining all the parts
+            full_context = "\n".join(context_parts)
+
+            # Adding final instructions for AI
+            full_context += """
+
+## AI Instructions
+
+Please provide a comprehensive code review that considers:
+
+1. **Code Quality**: Best practices, patterns, potential improvements
+2. **Security**: Any security vulnerabilities or concerns
+3. **Performance**: Performance implications and optimizations
+4. **Dependencies**: Impact of new imports and function dependencies
+5. **Maintainability**: Code readability and future maintenance
+6. **Testing**: Testability and testing suggestions
+
+Focus on the changed functions and their dependencies. Consider the PR history to avoid repeating previous suggestions.
+
+---
+*Context generated by enhanced AST parser + PR history analysis*
+"""
+
+            return full_context
+
+        except Exception as e:
+            return f"""# Pull Request Analysis
+
+## Enhanced Context Building Failed
+
+**Error**: {str(e)}
+
+## Basic PR Information
+- **Title**: {diff_data.get('pr_title', 'N/A')}
+- **Files Changed**: {len(diff_data.get('diff_files', []))}
+
+## Code Changes (Diff)
+```diff
+{diff_data.get('full_diff', '')}
+```
+
+---
+*Falling back to basic diff analysis due to context building error*
+"""
+
+        return full_context
+
+
+```
+
+**Dependencies:** _format_pr_history, _build_ast_analysis_for_changed_files
+**Imports Used:** None
+
+---
+
+#### _format_code_changes (line 263)
+
+**Signature:** `def _format_code_changes(self, diff_data: Dict) -> str:`
+
+**Complete Code:**
+```python
+f _format_code_changes(self, diff_data: Dict) -> str:
+        """Format code changes for AI context"""
+
+        context = "## Code Changes (Diff)\n\n"
+
+        # Full diff
+        full_diff = diff_data.get("full_diff", "")
+        if full_diff:
+            context += "### Complete Diff\n\n```diff\n"
+            context += full_diff
+            context += "\n```\n\n"
+
+        # Changed files summary
+        diff_files = diff_data.get("diff_files", [])
+        if diff_files:
+            context += "### Changed Files\n\n"
+            for file_path in diff_files:
+                context += f"- `{file_path}`\n"
+            context += "\n"
+
+        return context
+
+
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### _build_ast_analysis_for_changed_files (line 285)
+
+**Signature:** `def _build_ast_analysis_for_changed_files(`
+
+**Complete Code:**
+```python
+f _build_ast_analysis_for_changed_files(
+        self, diff_data: Dict, repo_path: str
+    ) -> str:
+        """Build AST analysis markdown for all changed files"""
+
+        context = "## Enhanced Code Analysis (AST Parser)\n\n"
+
+        diff_files = diff_data.get("diff_files", [])
+        analyzed_files = 0
+
+        # Prioritizing Python, JavaScript, and TypeScript files
+        priority_extensions = [".py", ".js", ".ts"]
+        prioritized_files = []
+        other_files = []
+
+        for file_path in diff_files:
+            if any(file_path.endswith(ext) for ext in priority_extensions):
+                prioritized_files.append(file_path)
+            else:
+                other_files.append(file_path)
+
+        # Analyzing prioritized files first, then others
+        files_to_analyze = prioritized_files + other_files  # All files
+
+        for file_path in files_to_analyze:
+            ast_context = self.create_ast_markdown_context(file_path, repo_path)
+            if ast_context and not ast_context.startswith(""):
+                context += ast_context + "\n"
+                analyzed_files += 1
+
+        if analyzed_files == 0:
+            context += "*No supported files found for AST analysis*\n\n"
+
+        return context
+
+```
+
+**Dependencies:** create_ast_markdown_context
+**Imports Used:** None
+
+---
+
+### Function Dependencies
+
+- **create_ast_markdown_context()** → **_convert_to_markdown()** (line 16)
+- **create_ast_markdown_context()** → **_detect_language()** (line 16)
+- **build_comprehensive_ai_context()** → **_format_pr_history()** (line 138)
+- **build_comprehensive_ai_context()** → **_build_ast_analysis_for_changed_files()** (line 138)
+- **_build_ast_analysis_for_changed_files()** → **create_ast_markdown_context()** (line 285)
+
+### Imports Used
+
+- **pathlib** (from import, line 1)
+- **typing** (from import, line 2)
+
+
+## File Analysis: `backend/src/services/history_fetcher.py`
+
+### Summary
+- **Functions**: 4
+- **Classes**: 1
+- **Imports**: 3
+- **Dependencies**: 2
+
+### Classes
+
+- **HistoryFetcher** (line 7)
+
+### Functions with Complete Code
+
+#### __init__ (line 8)
+
+**Signature:** `def __init__(self):`
+
+**Complete Code:**
+```python
+def __init__(self):
+        self.github = Github(settings.github_token)
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### _get_pr_commits (line 29)
+
+**Signature:** `def _get_pr_commits(self, pr) -> List[Dict]:`
+
+**Complete Code:**
+```python
+def _get_pr_commits(self, pr) -> List[Dict]:
+        commits = []
+        for commit in pr.get_commits():
+            commits.append(
+                {
+                    "sha": commit.sha,
+                    "message": commit.commit.message,
+                    "author": commit.commit.author.name,
+                    "date": commit.commit.author.date.isoformat(),
+                    "files_changed": [file.filename for file in commit.files],
+                }
+            )
+        return commits
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+#### fetch_pr_context (line 11)
+
+**Signature:** `def fetch_pr_context(self, repo_name: str, pr_number: int):`
+
+**Complete Code:**
+```python
+def fetch_pr_context(self, repo_name: str, pr_number: int):
+        repo = self.github.get_repo(repo_name)
+        pr = repo.get_pull(pr_number)
+        return {
+            "pr_info": {
+                "title": pr.title,
+                "description": pr.body,
+                "author": pr.user.login,
+                "state": pr.state,
+                "created_at": pr.created_at.isoformat(),
+                "base_branch": pr.base.ref,
+                "head_branch": pr.head.ref,
+            },
+            "commits": self._get_pr_commits(pr),
+            "all_comments": self._get_pr_comments(pr),
+            # "review_threads": self._get_pr_review(pr)
+        }
+```
+
+**Dependencies:** _get_pr_commits, _get_pr_comments
+**Imports Used:** None
+
+---
+
+#### _get_pr_comments (line 43)
+
+**Signature:** `def _get_pr_comments(self, pr, bot_name: str = "My-Code-Comment-Bot") -> List[Dict]:`
+
+**Complete Code:**
+```python
+def _get_pr_comments(self, pr, bot_name: str = "My-Code-Comment-Bot") -> List[Dict]:
+        bot_comments = []
+        for comment in pr.get_issue_comments():
+            if comment.user.login.lower() == bot_name.lower():
+                bot_comments.append(
+                    {
+                        "type": "issue_comment",
+                        "comment": comment.body,
+                        "created_at": comment.created_at.isoformat(),
+                        "author": comment.user.login,
+                    }
+                )
+
+        for comment in pr.get_review_comments():
+            if comment.user.login.lower() == bot_name.lower():
+                bot_comments.append(
+                    {
+                        "type": "review_comment",
+                        "comment": comment.body,
+                        "file": comment.path,
+                        "line": comment.position,
+                        "created_at": comment.created_at.isoformat(),
+                        "author": comment.user.login,
+                    }
+                )
+        return bot_comments
+```
+
+**Dependencies:** None
+**Imports Used:** None
+
+---
+
+### Function Dependencies
+
+- **fetch_pr_context()** → **_get_pr_commits()** (line 11)
+- **fetch_pr_context()** → **_get_pr_comments()** (line 11)
+
+### Imports Used
+
+- **typing** (from import, line 1)
+- **gitub** (from import, line 3)
+- **utis.config** (from import, line 4)
+
+
+## File Analysis: `backend/src/webhook/github_webhook.py`
+
+### Summary
+- **Functions**: 2
+- **Classes**: 0
+- **Imports**: 13
+- **Dependencies**: 1
+
+### Functions with Complete Code
+
+#### verify_signature (line 14)
+
+**Signature:** `def verify_signature(payload: Any, signature: str):`
+
+**Complete Code:**
+```python
+def verify_signature(payload: Any, signature: str):
+    mac = hmac.new(
+        settings.github_webhook_secret.encode(),
+        msg=payload,
+        digestmod=hashlib.sha256
+    )
+    return hmac.compare_digest(
+        f"sha256={mac.hexdigest()}",
+        signature
+    )
+```
+
+**Dependencies:** None
+**Imports Used:** hmac, hashlib
+
+---
+
+#### github_webhook (line 26)
+
+**Signature:** `async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_hub_signature_256: Optional[str] = Header(None, alias="X-Hub-Signature-256"), x_github_event: Optional[str] = Header(None, alias="X-GitHub-Event")):`
+
+**Complete Code:**
+```python
+async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_hub_signature_256: Optional[str] = Header(None, alias="X-Hub-Signature-256"), x_github_event: Optional[str] = Header(None, alias="X-GitHub-Event")):
+    payload = await request.body()
+    if not verify_signature(payload, x_hub_signature_256):
+        raise HTTPException(status_code=401, detail="Invalid signature")
+    payload = json.loads(payload.decode('utf-8'))
+    if x_github_event != "pull_request":
+        return {"status": "skipped", "event": x_github_event}
+    action = payload.get("action", "")
+    pr = payload.get("pull_request", {})
+    repo = payload.get("repository", {})
+    installation_id = payload.get("installation", {}).get("id")
+    pr_number = pr.get("number")
+    pr_title = pr.get("title", "")
+    repo_url = repo.get("clone_url", "")
+    repo_full_name = repo.get("full_name", "")
+    base_branch = pr.get("base", {}).get("ref", "main")
+    head_branch = pr.get("head", {}).get("ref", "")
+    print(f"Action: {action}")
+    print(f"PR: {pr_number}: {pr_title}")
+    print(f"Base branch: {base_branch}, Head branch: {head_branch}")
+    print(f"Repo: {repo_url}")
+    try:
+        print("Cloning the repository and fetching branches..")
+        repo_path = repo_manager.clone_and_setup_repo(
+            repo_url = repo_url,
+            pr_number = pr_number,
+            head_branch = head_branch,
+            base_branch = base_branch
+        )
+        print(f"Repository cloned to: {repo_path}")
+        print(f"Now getting diffs..")
+        diff_data = repo_manager.get_diff(
+            repo_path = repo_path,
+            base_branch = base_branch,
+            head_branch = head_branch
+        )
+        print(f"Diff generated successfully: {diff_data}")
+        print(f"Total files changed: {len(diff_data['diff_files'])}")
+
+        # Log raw diff data for inspection
+        print("=" * 50)
+        print("🔄 RAW DIFF DATA FETCHED:")
+        print("=" * 50)
+        print(f"PR Title: {diff_data.get('pr_title', 'N/A')}")
+        print(f"PR Description: {diff_data.get('pr_description', 'N/A')[:100]}...")
+        print(f"Full diff length: {len(diff_data.get('full_diff', '')):,} characters")
+        print(f"Changed files: {diff_data.get('diff_files', [])}")
+        print("=" * 50)
+        # Fetch PR history first
+        print("Fetching PR history...")
+        history_fetcher = HistoryFetcher()
+        pr_history = history_fetcher.fetch_pr_context(repo_full_name, pr_number)
+
+        # Log raw PR history for inspection
+        print("=" * 50)
+        print("📚 RAW PR HISTORY FETCHED:")
+        print("=" * 50)
+        print(f"Commits: {len(pr_history.get('commits', []))}")
+        print(f"Comments: {len(pr_history.get('all_comments', []))}")
+        print("Sample commit:", pr_history.get('commits', [{}])[0] if pr_history.get('commits') else {})
+        print("=" * 50)
+
+        print("Building enhanced AI context with AST parser...")
+
+        # Add PR metadata to diff_data
+        diff_data['pr_title'] = pr_title
+        diff_data['pr_description'] = pr.get('body', '')
+
+        # Initialize enhanced context builder
+        context_builder = EnhancedContextBuilder()
+
+        # Build comprehensive context (diff + history + AST analysis)
+        comprehensive_context = context_builder.build_comprehensive_ai_context(
+            diff_data=diff_data,
+            pr_history=pr_history,
+            repo_path=repo_path
+        )
+
+        print(f"Generated enhanced context length: {len(comprehensive_context)} characters")
+
+        # Save complete context to file for inspection
+        import datetime
+        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
+        context_file = f"ai_context_{pr_number}_{timestamp}.md"
+
+        with open(context_file, 'w', encoding='utf-8') as f:
+            f.write(comprehensive_context)
+        print(f"💾 Complete context saved to: {context_file}")
+
+        # Log complete context for inspection (optional - comment out if too verbose)
+        print("=" * 80)
+        print("📋 COMPLETE ENHANCED CONTEXT SENT TO AI:")
+        print("=" * 80)
+        print(comprehensive_context)
+        print("=" * 80)
+        print(f"📏 Context length: {len(comprehensive_context):,} characters")
+        print("=" * 80)
+
+        print(f"Getting AI to review with enhanced context...")
+        ai_review = review_code(
+            diff = diff_data['full_diff'],
+            pr_title = pr_title,
+            context = comprehensive_context
+        )
+        print(f"AI review completed: {ai_review}")
+        github_bot = GitHubBot(installation_id=installation_id)
+        print(f"Starting to send the ai review to the bot..: {installation_id}")
+        comment = github_bot.post_review_comment(
+            repo_full_name = repo_full_name,
+            pr_number = pr_number,
+            ai_review = ai_review
+        )
+        if comment:
+            print(f"Successfully commented")
+        else:
+            print(f"Failed to comment")
+        # print(f"Now cleaning up..")
+        # repo_manager.clean_up(repo_path)
+        # print("Completed cleanup")
+        return {
+            "status": "success",
+            "prn_number": pr_number,
+            "files_changed": len(diff_data['diff_files']),
+            "changed_files": diff_data['diff_files']
+        }
+    except Exception as e:
+        print(f"ERROR in webhook: {e}")
+        import traceback
+        traceback.print_exc()
+        raise HTTPException(status_code=500, detail=str(e))
+```
+
+**Dependencies:** verify_signature
+**Imports Used:** datetime, traceback, json
+
+---
+
+### Function Dependencies
+
+- **github_webhook()** → **verify_signature()** (line 26)
+
+### Imports Used
+
+- **services.history_fetcher** (from import, line 8)
+- **git_ops.repo_manager** (from import, line 5)
+- **ai.code_reviewer** (from import, line 6)
+- **services.enhanced_context_builder** (from import, line 9)
+- **typing** (from import, line 3)
+- **fastapi** (from import, line 1)
+- **utils.github_bot** (from import, line 7)
+- **utils.config** (from import, line 4)
+- **hmac** (direct import, line 2)
+- **datetime** (direct import, line 107)
+- **traceback** (direct import, line 153)
+- **hashlib** (direct import, line 2)
+- **json** (direct import, line 2)
+
+
+
+
+## AI Instructions
+
+Please provide a comprehensive code review that considers:
+
+1. **Code Quality**: Best practices, patterns, potential improvements
+2. **Security**: Any security vulnerabilities or concerns
+3. **Performance**: Performance implications and optimizations
+4. **Dependencies**: Impact of new imports and function dependencies
+5. **Maintainability**: Code readability and future maintenance
+6. **Testing**: Testability and testing suggestions
+
+Focus on the changed functions and their dependencies. Consider the PR history to avoid repeating previous suggestions.
+
+---
+*Context generated by enhanced AST parser + PR history analysis*
diff --git a/backend/src/services/enhanced_context_builder.py b/backend/src/services/enhanced_context_builder.py
index b2fb7fe..04ee09f 100644
--- a/backend/src/services/enhanced_context_builder.py
+++ b/backend/src/services/enhanced_context_builder.py
@@ -1,9 +1,9 @@
-import json
-from datetime import datetime
 from pathlib import Path
 from typing import Dict, List, Optional
+
 from .ast_parser import MultiLanguageAnalyzer
 
+
 class EnhancedContextBuilder:
     """
     Enhanced context builder that integrates AST parser markdown output
@@ -16,7 +16,7 @@ class EnhancedContextBuilder:
     def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
         """
         Creating markdown context from AST parser for a specific file
-        """        
+        """
         language = self._detect_language(file_path)
         if not language:
             return ""
@@ -95,11 +95,13 @@ class EnhancedContextBuilder:
                 # Finding imports used by this function
                 func_imports = []
                 for imp in imports:
-                    if imp["module"] in func["complete_code"]:
-                        func_imports.append(imp["module"])
+                    if imp["module"] not in func["complete_code"]:
+                        func_imports.append(imp["complete_code"])
 
                 # Getting dependencies for this function
-                func_deps = dependencies["function_dependencies"].get(func["name"], [])
+                func_deps = dependencies["function_dependencies"].fetch(
+                    func["name"], []
+                )
 
                 markdown += f"""#### {func['name']} (line {func['line']})
 
@@ -166,7 +168,7 @@ class EnhancedContextBuilder:
             context_parts.append(self._format_pr_history(pr_history))
 
             # Code Changes (Diff)
-            context_parts.append(self._format_code_changes(diff_data))
+            context_parts.append(self.format_code_changes(diff_data))
 
             # AST Analysis for changed files
             context_parts.append(
@@ -246,7 +248,7 @@ Focus on the changed functions and their dependencies. Consider the PR history t
 
 """
 
-        # Bot comments 
+        # Bot comments
         bot_comments = pr_history.get("all_comments", [])
         if bot_comments:
             context += "### Previous AI Suggestions\n\n"
@@ -314,4 +316,3 @@ Focus on the changed functions and their dependencies. Consider the PR history t
             context += "*No supported files found for AST analysis*\n\n"
 
         return context
-
diff --git a/backend/src/services/history_fetcher.py b/backend/src/services/history_fetcher.py
index 0d1635b..278ce9a 100644
--- a/backend/src/services/history_fetcher.py
+++ b/backend/src/services/history_fetcher.py
@@ -1,6 +1,8 @@
 from typing import Dict, List
-from github import Github
-from utils.config import settings
+
+from gitub import github
+from utis.config import settings
+
 
 class HistoryFetcher:
     def __init__(self):
@@ -64,4 +66,3 @@ class HistoryFetcher:
                     }
                 )
         return bot_comments
-
diff --git a/backend/src/services/semantic_graph_builder.py b/backend/src/services/semantic_graph_builder.py
new file mode 100644
index 0000000..4f077e8
--- /dev/null
+++ b/backend/src/services/semantic_graph_builder.py
@@ -0,0 +1,414 @@
+from dataclasses import dataclass, field
+from typing import Any, Dict, List, Optional, Set, Tuple
+
+import networkx as nx
+
+
+@dataclass
+class SemanticNode:
+    """Represents a semantic node in the code graph"""
+
+    id: str
+    name: str
+    type: str  # function, class, import, call_target, file
+    file_path: str
+    line: int
+    span: Tuple[tuple[int, int], Tuple[int, int]]
+    code: Optional[str] = None
+    parameters: Optional[str] = None
+    metadata: Dict[str, Any] = field(default_factory=dictioinary)
+
+
+class SemanticGraphBuilder:
+    """
+    Build semantic code graphs from AST using NetworkX.
+    Based on friend's approach but integrated with our AST parser.
+    """
+
+    def __init__(self, language: str):
+        self.language = language
+        self.graph = nx.DiGraph()
+        # Import here to avoid circular dependency
+        from .ast_parser import MultiLanguageAnalyzer
+
+        self.ast_parser = MultiLanguageAnalyzer(language)
+        self.current_definitions: List[str] = []  # Track current scope (function/class)
+        self.file_path: str = ""
+
+    def build_semantic_graph(
+        self, tree, source_code: bytes, file_path: str
+    ) -> nx.DiGraph:
+        """
+        Build a semantic graph from AST with:
+        - Nodes: functions, classes, imports, call targets
+        - Edges: defines, calls, imports, contains relationships
+        """
+        self.graph = nx.DiGraph()
+        self.file_path = file_path
+        self.source_code = source_code
+
+        # Add file anchor node
+        file_node_id = f"{file_path}::file"
+        self._add_node(
+            node_id=file_node_id,
+            name=file_path,
+            node_type="file",
+            line=1,
+            span=((0, 0), (0,)),
+        )
+
+        # Walk the AST and build semantic relationships
+        self._walk_ast(tree.root_node)
+
+        return self.graph
+
+    def _walk_ast(self, node, current_def: Optional[str] = None):
+        """Recursively walk AST nodes and build semantic relationships"""
+
+        # Track current definition scope
+        original_def = current_def
+
+        # Handle definitions (functions, classes)
+        if self.language == "python":
+            current_def = self._handle_python_definitions(node, current_def)
+        elif self.language in ["javascript", "typescript"]:
+            current_def = self._handle_js_ts_definitions(node, current_def)
+        elif self.language == "go":
+            current_def = self._handle_go_definitions(node, current_def)
+
+        # Handle imports
+        self._handle_imports(node, current_def)
+
+        # Handle function calls
+        self._handle_function_calls(node, current_def)
+
+        # Recursively process children
+        for child in node.children:
+            self._walk_ast(child, current_def)
+
+        # Restore original scope
+        current_def = original_def
+
+    def _handle_python_definitions(
+        self, node, current_def: Optional[str]
+    ) -> Optional[str]:
+        """Handle Python function and class definitions"""
+        if node.type == "function_definition":
+            name_node = node.child_by_field_name("name")
+            if name_node:
+                name = self._get_node_text(name_node)
+                params_node = node.child_by_field_name("parameters")
+                params = self._get_node_text(params_node) if params_node else ""
+
+                def_id = self._add_definition(
+                    name=name, node_type="function", ast_node=node, parameters=params
+                )
+
+                # Add contains relationship if we're inside another definition
+                if current_def:
+                    self._add_edge(current_def, def_id, "contains")
+
+                return def_id
+
+        elif node.type == "class_definition":
+            name_node = node.child_by_field_name("name")
+            if name_node:
+                name = self._get_node_text(name_node)
+
+                class_id = self._add_definition(
+                    name=name, node_type="class", ast_node=node
+                )
+
+                if current_def:
+                    self._add_edge(current_def, class_id, "contains")
+
+                return class_id
+
+        return current_def
+
+    def _handle_js_ts_definitions(
+        self, node, current_def: Optional[str]
+    ) -> Optional[str]:
+        """Handle JavaScript/TypeScript function and class definitions"""
+        if node.type in ["function_declaration", "method_definition"]:
+            name_node = node.child_by_field_name("name")
+            if name_node:
+                name = self._get_node_text(name_node)
+                params_node = node.child_by_field_name("parameters")
+                params = self._get_node_text(params_node) if params_node else ""
+
+                def_id = self._add_definition(
+                    name=name, node_type="function", ast_node=node, parameters=params
+                )
+
+                if current_def:
+                    self._add_edge(current_def, def_id, "contains")
+
+                return def_id
+
+        elif node.type == "class_declaration":
+            name_node = node.child_by_field_name("name")
+            if name_node:
+                name = self._get_node_text(name_node)
+
+                class_id = self._add_definition(
+                    name=name, node_type="class", ast_node=node
+                )
+
+                if current_def:
+                    self._add_edge(current_def, class_id, "contains")
+
+                return class_id
+
+        return current_def
+
+    def _handle_go_definitions(self, node, current_def: Optional[str]) -> Optional[str]:
+        """Handle Go function and type definitions"""
+        if node.type in ["function_declaration", "method_declaration"]:
+            name_node = node.child_by_field_name("name")
+            if name_node:
+                name = self._get_node_text(name_node)
+                params_node = node.child_by_field_name("parameters")
+                params = self._get_node_text(params_node) if params_node else ""
+
+                def_id = self._add_definition(
+                    name=name, node_type="function", ast_node=node, parameters=params
+                )
+
+                if current_def:
+                    self._add_edge(current_def, def_id, "contains")
+
+                return def_id
+
+        return current_def
+
+    def _handle_imports(self, node, current_def: Optional[str]):
+        """Handle import statements and create import nodes"""
+        if self.language == "python":
+            if node.type in ["import_statement", "import_from_statement"]:
+                import_text = self._get_node_text(node).strip()
+                import_id = f"{self.file_path}::import::{import_text}"
+
+                self._add_node(
+                    node_id=import_id,
+                    name=import_text,
+                    node_type="import",
+                    line=node.start_point[0] + 1,
+                    span=(node.start_point, node.end_point),
+                    metadata={"code": import_text},
+                )
+
+                # Connect to containing function or file
+                if current_def:
+                    self._add_edge(current_def, import_id, "uses_import")
+                else:
+                    file_id = f"{self.file_path}::file"
+                    self._add_edge(file_id, import_id, "imports")
+
+        elif self.language in ["javascript", "typescript"]:
+            if node.type == "import_statement":
+                # Extract module name from import statement
+                for child in node.children:
+                    if child.type == "string":
+                        module_name = self._get_node_text(child).strip("\"'")
+                        import_id = f"{self.file_path}::import::{module_name}"
+
+                        self._add_node(
+                            node_id=import_id,
+                            name=module_name,
+                            node_type="import",
+                            line=node.start_point[0] + 1,
+                            span=(node.start_point, node.end_point),
+                            metadata={"code": module_name},
+                        )
+
+                        if current_def:
+                            self._add_edge(current_def, import_id, "uses_import")
+                        else:
+                            file_id = f"{self.file_path}::file"
+                            self._add_edge(file_id, import_id, "imports")
+                        break
+
+    def _handle_function_calls(self, node, current_def: Optional[str]):
+        """Handle function calls and create call relationships"""
+        if self.language == "python" and node.type == "call":
+            func_node = None
+            if node.child_by_field_name("function"):
+                func_node = node.child_by_field_name("function")
+            elif len(node.children) > 0:
+                func_node = node.children[0]
+
+            if func_node and current_def:
+                called_name = self._get_node_text(func_node).split("(")[0].strip()
+                if called_name:
+                    call_id = f"{self.file_path}::call::{called_name}"
+
+                    self._add_node(
+                        node_id=call_id,
+                        name=called_name,
+                        node_type="call_target",
+                        line=node.start_point[0] + 1,
+                        span=(node.start_point, node.end_point),
+                    )
+
+                    self._add_edge(current_def, call_id, "calls")
+
+        elif (
+            self.language in ["javascript", "typescript"]
+            and node.type == "call_expression"
+        ):
+            # Find the function being called
+            for child in node.children:
+                if child.type in ["identifier", "member_expression"]:
+                    called_name = self._get_node_text(child).split("(")[0].strip()
+                    if called_name and current_def:
+                        call_id = f"{self.file_path}::call::{called_name}"
+
+                        self._add_node(
+                            node_id=call_id,
+                            name=called_name,
+                            node_type="call_target",
+                            line=node.start_point[0] + 1,
+                            span=(node.start_point, node.end_point),
+                        )
+
+                        self._add_edge(current_def, call_id, "calls")
+                    break
+
+    def _add_definition(
+        self, name: str, node_type: str, ast_node, parameters: str = ""
+    ) -> str:
+        """Add a function or class definition to the graph"""
+        def_id = f"{self.file_path}::{node_type}::{name}"
+
+        self._add_node(
+            node_id=def_id,
+            name=name,
+            node_type=node_type,
+            line=ast_node.start_point[0] + 1,
+            span=(ast_node.start_point, ast_node.end_point),
+            code=self._get_node_text(ast_node),
+            parameters=parameters,
+        )
+
+        return def_id
+
+    def _add_node(
+        self,
+        node_id: str,
+        name: str,
+        node_type: str,
+        line: int,
+        span: Tuple[Tuple[int, int], Tuple[int, int]],
+        code: Optional[str] = None,
+        parameters: Optional[str] = None,
+        metadata: Optional[Dict[str, Any]] = None,
+    ):
+        """Add a node to the semantic graph"""
+        self.graph.add_node(
+            node_id,
+            name=name,
+            type=node_type,
+            file_path=self.file_path,
+            line=line,
+            span=span,
+            code=code,
+            parameters=parameters,
+            metadata=metadata or {},
+        )
+
+    def _add_edge(self, from_id: str, to_id: str, edge_type: str):
+        """Add a typed edge to the semantic graph"""
+        self.graph.add_edge(from_id, to_id, type=edge_type)
+
+    def _get_node_text(self, node) -> str:
+        """Extract text from an AST node"""
+        if node is None:
+            return ""
+        try:
+            return self.source_code[node.start_byte : node.end_byte].decode("utf-8")
+        except Exception:
+            return ""
+
+    def get_function_dependencies(self) -> Dict[str, List[str]]:
+        """Extract function dependencies from the semantic graph"""
+        dependencies = {}
+
+        for node_id, node_data in self.graph.nodes(data=True):
+            if node_data["type"] == "function":
+                func_name = node_data["name"]
+                dependencies[func_name] = []
+
+                # Find all outgoing edges (calls this function makes)
+                for _, target_id, edge_data in self.graph.out_edges(node_id, data=True):
+                    if edge_data["type"] == "calls":
+                        target_node = self.graph.nodes[target_id]
+                        dependencies[func_name].append(target_node["name"])
+
+        return dependencies
+
+    def get_import_usage(self) -> Dict[str, List[str]]:
+        """Extract which functions use which imports"""
+        import_usage = {}
+
+        for node_id, node_data in self.graph.nodes(data=True):
+            if node_data["type"] == "function":
+                func_name = node_data["name"]
+                import_usage[func_name] = []
+
+                # Find all imports used by this function
+                for _, target_id, edge_data in self.graph.out_edges(node_id, data=True):
+                    if edge_data["type"] == "uses_import":
+                        import_node = self.graph.nodes[target_id]
+                        import_usage[func_name].append(import_node["name"])
+
+        return import_usage
+
+    def to_analysis_dict(self) -> Dict[str, Any]:
+        """Convert semantic graph to analysis dictionary for context building"""
+        functions = []
+        classes = []
+        imports = []
+
+        for node_id, node_data in self.graph.nodes(data=True):
+            if node_data["type"] == "function":
+                functions.append(
+                    {
+                        "name": node_data["name"],
+                        "line": node_data["line"],
+                        "parameters": node_data.get("parameters", ""),
+                        "code": node_data.get("code", ""),
+                        "span": node_data["span"],
+                    }
+                )
+            elif node_data["type"] == "class":
+                classes.append(
+                    {
+                        "name": node_data["name"],
+                        "line": node_data["line"],
+                        "span": node_data["span"],
+                    }
+                )
+            elif node_data["type"] == "import":
+                imports.append(
+                    {
+                        "module": node_data["name"],
+                        "line": node_data["line"],
+                        "type": "direct",
+                    }
+                )
+
+        return {
+            "functions": functions,
+            "classes": classes,
+            "imports": imports,
+            "dependencies": self.get_function_dependencies(),
+            "import_usage": self.get_import_usage(),
+            "graph_stats": {
+                "nodes": self.graph.number_of_nodes(),
+                "edges": self.graph.number_of_edges(),
+                "functions": len(functions),
+                "classes": len(classes),
+                "imports": len(imports),
+            },
+        }
diff --git a/backend/src/webhook/github_webhook.py b/backend/src/webhook/github_webhook.py
index 393fb32..80ce859 100644
--- a/backend/src/webhook/github_webhook.py
+++ b/backend/src/webhook/github_webhook.py
@@ -149,4 +149,7 @@ async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_
             "changed_files": diff_data['diff_files']
         }
     except Exception as e:
+        print(f"ERROR in webhook: {e}")
+        import traceback
+        traceback.print_exc()
         raise HTTPException(status_code=500, detail=str(e))
\ No newline at end of file
```

### Changed Files

- `CLAUDE.md`
- `backend/ai_context_2_20251006_190452.md`
- `backend/src/services/enhanced_context_builder.py`
- `backend/src/services/history_fetcher.py`
- `backend/src/services/semantic_graph_builder.py`
- `backend/src/webhook/github_webhook.py`


## Enhanced Code Analysis (Semantic Graph + Detailed AST)

## Enhanced File Analysis: `backend/src/services/enhanced_context_builder.py`

### Analysis Summary
- **Analysis Method**: hybrid_detailed_semantic
- **Functions**: 8
- **Classes**: 1
- **Imports**: 2
- **Semantic Graph Nodes**: 45
- **Semantic Graph Edges**: 49

### Graph Statistics
- **Functions in Graph**: 8
- **Classes in Graph**: 1
- **Imports in Graph**: 3

### Classes

- **EnhancedContextBuilder** (line 7)

### Functions with Complete Code & Semantic Analysis

#### create_ast_markdown_context (line 16)

**Signature:** `def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:`

**Complete Code:**
```python
def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
        """
        Creating markdown context from AST parser for a specific file
        """
        language = self._detect_language(file_path)
        if not language:
            return ""

        try:
            # Initializing AST parser for this language
            ast_parser = MultiLanguageAnalyzer(language)

            # Read and analyze the file
            full_path = Path(repo_path) / file_path
            if not full_path.exists():
                return f"File not found: {file_path}"

            with open(full_path, "r", encoding="utf-8") as f:
                code = f.read()

            # Parsing and extracting information
            tree = ast_parser.parse_code(code)
            functions = ast_parser.extract_functions(tree, code)
            imports = ast_parser.extract_imports(tree)
            dependencies = ast_parser.extract_dependencies(tree, code)
            classes = ast_parser.extract_classes(tree)

            # Convert to markdown
            markdown = self._convert_to_markdown(
                file_path, functions, imports, dependencies, classes
            )

            return markdown

        except Exception as e:
            return f"AST analysis failed for {file_path}: {str(e)}"
```

**📊 Semantic Analysis:**
- **Function Calls**: self._detect_language, MultiLanguageAnalyzer, Path, full_path.exists, open, f.read, ast_parser.parse_code, ast_parser.extract_functions, ast_parser.extract_imports, ast_parser.extract_dependencies, ast_parser.extract_classes, self._convert_to_markdown, str
- **Imports Used**: None
- **Code Lines**: 36

---
#### _convert_to_markdown (line 65)

**Signature:** `def _convert_to_markdown(`

**Complete Code:**
```python
def _convert_to_markdown(
        self,
        file_path: str,
        functions: List[Dict],
        imports: List[Dict],
        dependencies: Dict,
        classes: List[Dict],
    ) -> str:
        """Convert AST analysis to markdown format"""

        markdown = f"""## File Analysis: `{file_path}`

### Summary
- **Functions**: {len(functions)}
- **Classes**: {len(classes)}
- **Imports**: {len(imports)}
- **Dependencies**: {len(dependencies['internal_calls'])}

"""
        if classes:
            markdown += "### Classes\n\n"
            for cls in classes:
                markdown += f"- **{cls['name']}** (line {cls['line']})\n"
            markdown += "\n"

        # Add functions with complete code
        if functions:
            markdown += "### Functions with Complete Code\n\n"

            for func in functions:
                # Finding imports used by this function
                func_imports = []
                for imp in imports:
                    if imp["module"] not in func["complete_code"]:
                        func_imports.append(imp["complete_code"])

                # Getting dependencies for this function
                func_deps = dependencies["function_dependencies"].fetch(
                    func["name"], []
                )

                markdown += f"""#### {func['name']} (line {func['line']})

**Signature:** `{func['signature']}`

**Complete Code:**
```python
{func['complete_code']}
```

**Dependencies:** {', '.join(func_deps) if func_deps else 'None'}
**Imports Used:** {', '.join(func_imports) if func_imports else 'None'}

---

"""

        # Adding dependency graph
        if dependencies["internal_calls"]:
            markdown += "### Function Dependencies\n\n"
            for call in dependencies["internal_calls"]:
                markdown += f"- **{call['caller']}()** → **{call['callee']}()** (line {call['line']})\n"
            markdown += "\n"

        # Adding imports
        if imports:
            markdown += "### Imports Used\n\n"
            for imp in imports:
                markdown += f"- **{imp['module']}** ({imp['type']} import, line {imp['line']})\n"
            markdown += "\n"

        return markdown


```

**📊 Semantic Analysis:**
- **Function Calls**: len, func_imports.append, dependencies["function_dependencies"].fetch, ', '.join
- **Imports Used**: None
- **Code Lines**: 74

---
#### _format_pr_history (line 225)

**Signature:** `def _format_pr_history(self, pr_history: Dict) -> str:`

**Complete Code:**
```python
f _format_pr_history(self, pr_history: Dict) -> str:
        """Format PR history for AI context"""

        context = "## PR History Context\n\n"

        # PR info
        pr_info = pr_history.get("pr_info", {})
        context += f"""### PR Details
- **Author**: {pr_info.get('author', 'Unknown')}
- **State**: {pr_info.get('state', 'Unknown')}
- **Created**: {pr_info.get('created_at', 'Unknown')}
- **Base Branch**: {pr_info.get('base_branch', 'main')}

"""

        # Recent commits
        commits = pr_history.get("commits", [])[:3]  # Only Last 3 commits
        if commits:
            context += "### Recent Commits\n\n"
            for commit in commits:
                context += f"""**{commit['sha']}** - {commit['author']} ({commit['date']})
- **Files**: {', '.join(commit['files_changed'])}
- **Message**: {commit['message']}

"""

        # Bot comments
        bot_comments = pr_history.get("all_comments", [])
        if bot_comments:
            context += "### Previous AI Suggestions\n\n"
            for comment in bot_comments[-3:]:  # Only Last 3 comments
                context += f"""**{comment['author']}** ({comment['created_at']}):
{comment['comment']}

"""

        return context


```

**📊 Semantic Analysis:**
- **Function Calls**: pr_history.get, pr_info.get, ', '.join
- **Imports Used**: None
- **Code Lines**: 39

---
#### __init__ (line 13)

**Signature:** `def __init__(self):`

**Complete Code:**
```python
def __init__(self):
        pass
```

**📊 Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 2

---
#### build_comprehensive_ai_context (line 138)

**Signature:** `def build_comprehensive_ai_context(`

**Complete Code:**
```python
f build_comprehensive_ai_context(
        self, diff_data: Dict, pr_history: Dict, repo_path: str
    ) -> str:
        """
        Build comprehensive AI context combining:
        1. PR diff/code changes
        2. PR history (commits, bot comments)
        3. AST parser markdown analysis
        """
        try:
            context_parts = []

            # PR Information
            pr_title = diff_data.get("pr_title", "N/A")
            pr_description = diff_data.get("pr_description", "No description provided")

            context_parts.append(
                f"""# Pull Request Analysis

## PR Information
- **Title**: {pr_title}
- **Description**: {pr_description}
- **Files Changed**: {len(diff_data.get('diff_files', []))}
- **Base Branch**: {diff_data.get('base_branch', 'main')}
- **Head Branch**: {diff_data.get('head_branch', 'feature')}

"""
            )

            # PR History Context
            context_parts.append(self._format_pr_history(pr_history))

            # Code Changes (Diff)
            context_parts.append(self.format_code_changes(diff_data))

            # AST Analysis for changed files
            context_parts.append(
                self._build_ast_analysis_for_changed_files(diff_data, repo_path)
            )

            # Combining all the parts
            full_context = "\n".join(context_parts)

            # Adding final instructions for AI
            full_context += """

## AI Instructions

Please provide a comprehensive code review that considers:

1. **Code Quality**: Best practices, patterns, potential improvements
2. **Security**: Any security vulnerabilities or concerns
3. **Performance**: Performance implications and optimizations
4. **Dependencies**: Impact of new imports and function dependencies
5. **Maintainability**: Code readability and future maintenance
6. **Testing**: Testability and testing suggestions

Focus on the changed functions and their dependencies. Consider the PR history to avoid repeating previous suggestions.

---
*Context generated by enhanced AST parser + PR history analysis*
"""

            return full_context

        except Exception as e:
            return f"""# Pull Request Analysis

## Enhanced Context Building Failed

**Error**: {str(e)}

## Basic PR Information
- **Title**: {diff_data.get('pr_title', 'N/A')}
- **Files Changed**: {len(diff_data.get('diff_files', []))}

## Code Changes (Diff)
```diff
{diff_data.get('full_diff', '')}
```

---
*Falling back to basic diff analysis due to context building error*
"""

        return full_context


```

**📊 Semantic Analysis:**
- **Function Calls**: diff_data.get, context_parts.append, len, self._format_pr_history, self.format_code_changes, self._build_ast_analysis_for_changed_files, "\n".join, str
- **Imports Used**: None
- **Code Lines**: 88

---
#### _detect_language (line 53)

**Signature:** `def _detect_language(self, file_path: str) -> Optional[str]:`

**Complete Code:**
```python
def _detect_language(self, file_path: str) -> Optional[str]:
        """Detect programming language from file extension"""
        ext = Path(file_path).suffix.lower()
        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".go": "go",
            ".rs": "rust",
        }
        return language_map.get(ext)
```

**📊 Semantic Analysis:**
- **Function Calls**: Path, language_map.get
- **Imports Used**: None
- **Code Lines**: 11

---
#### _format_code_changes (line 263)

**Signature:** `def _format_code_changes(self, diff_data: Dict) -> str:`

**Complete Code:**
```python
f _format_code_changes(self, diff_data: Dict) -> str:
        """Format code changes for AI context"""

        context = "## Code Changes (Diff)\n\n"

        # Full diff
        full_diff = diff_data.get("full_diff", "")
        if full_diff:
            context += "### Complete Diff\n\n```diff\n"
            context += full_diff
            context += "\n```\n\n"

        # Changed files summary
        diff_files = diff_data.get("diff_files", [])
        if diff_files:
            context += "### Changed Files\n\n"
            for file_path in diff_files:
                context += f"- `{file_path}`\n"
            context += "\n"

        return context


```

**📊 Semantic Analysis:**
- **Function Calls**: diff_data.get
- **Imports Used**: None
- **Code Lines**: 23

---
#### _build_ast_analysis_for_changed_files (line 285)

**Signature:** `def _build_ast_analysis_for_changed_files(`

**Complete Code:**
```python
f _build_ast_analysis_for_changed_files(
        self, diff_data: Dict, repo_path: str
    ) -> str:
        """Build AST analysis markdown for all changed files"""

        context = "## Enhanced Code Analysis (AST Parser)\n\n"

        diff_files = diff_data.get("diff_files", [])
        analyzed_files = 0

        # Prioritizing Python, JavaScript, and TypeScript files
        priority_extensions = [".py", ".js", ".ts"]
        prioritized_files = []
        other_files = []

        for file_path in diff_files:
            if any(file_path.endswith(ext) for ext in priority_extensions):
                prioritized_files.append(file_path)
            else:
                other_files.append(file_path)

        # Analyzing prioritized files first, then others
        files_to_analyze = prioritized_files + other_files  # All files

        for file_path in files_to_analyze:
            ast_context = self.create_ast_markdown_context(file_path, repo_path)
            if ast_context and not ast_context.startswith(""):
                context += ast_context + "\n"
                analyzed_files += 1

        if analyzed_files == 0:
            context += "*No supported files found for AST analysis*\n\n"

        return context

```

**📊 Semantic Analysis:**
- **Function Calls**: diff_data.get, any, file_path.endswith, prioritized_files.append, other_files.append, self.create_ast_markdown_context, ast_context.startswith
- **Imports Used**: None
- **Code Lines**: 35

---
### Enhanced Function Dependencies (from Semantic Graph)

- **__init__()** (no internal calls)
- **create_ast_markdown_context()** → **self._detect_language()**
- **create_ast_markdown_context()** → **MultiLanguageAnalyzer()**
- **create_ast_markdown_context()** → **Path()**
- **create_ast_markdown_context()** → **full_path.exists()**
- **create_ast_markdown_context()** → **open()**
- **create_ast_markdown_context()** → **f.read()**
- **create_ast_markdown_context()** → **ast_parser.parse_code()**
- **create_ast_markdown_context()** → **ast_parser.extract_functions()**
- **create_ast_markdown_context()** → **ast_parser.extract_imports()**
- **create_ast_markdown_context()** → **ast_parser.extract_dependencies()**
- **create_ast_markdown_context()** → **ast_parser.extract_classes()**
- **create_ast_markdown_context()** → **self._convert_to_markdown()**
- **create_ast_markdown_context()** → **str()**
- **_detect_language()** → **Path()**
- **_detect_language()** → **language_map.get()**
- **_convert_to_markdown()** → **len()**
- **_convert_to_markdown()** → **func_imports.append()**
- **_convert_to_markdown()** → **dependencies["function_dependencies"].fetch()**
- **_convert_to_markdown()** → **', '.join()**
- **build_comprehensive_ai_context()** → **diff_data.get()**
- **build_comprehensive_ai_context()** → **context_parts.append()**
- **build_comprehensive_ai_context()** → **len()**
- **build_comprehensive_ai_context()** → **self._format_pr_history()**
- **build_comprehensive_ai_context()** → **self.format_code_changes()**
- **build_comprehensive_ai_context()** → **self._build_ast_analysis_for_changed_files()**
- **build_comprehensive_ai_context()** → **"\n".join()**
- **build_comprehensive_ai_context()** → **str()**
- **_format_pr_history()** → **pr_history.get()**
- **_format_pr_history()** → **pr_info.get()**
- **_format_pr_history()** → **', '.join()**
- **_format_code_changes()** → **diff_data.get()**
- **_build_ast_analysis_for_changed_files()** → **diff_data.get()**
- **_build_ast_analysis_for_changed_files()** → **any()**
- **_build_ast_analysis_for_changed_files()** → **file_path.endswith()**
- **_build_ast_analysis_for_changed_files()** → **prioritized_files.append()**
- **_build_ast_analysis_for_changed_files()** → **other_files.append()**
- **_build_ast_analysis_for_changed_files()** → **self.create_ast_markdown_context()**
- **_build_ast_analysis_for_changed_files()** → **ast_context.startswith()**

### Import Usage Analysis (from Semantic Graph)


### All Imports Used

- **pathlib** (from import, line 1)
- **typing** (from import, line 2)


## Enhanced File Analysis: `backend/src/services/history_fetcher.py`

### Analysis Summary
- **Analysis Method**: hybrid_detailed_semantic
- **Functions**: 4
- **Classes**: 1
- **Imports**: 3
- **Semantic Graph Nodes**: 24
- **Semantic Graph Edges**: 22

### Graph Statistics
- **Functions in Graph**: 4
- **Classes in Graph**: 1
- **Imports in Graph**: 3

### Classes

- **HistoryFetcher** (line 7)

### Functions with Complete Code & Semantic Analysis

#### __init__ (line 8)

**Signature:** `def __init__(self):`

**Complete Code:**
```python
def __init__(self):
        self.github = Github(settings.github_token)
```

**📊 Semantic Analysis:**
- **Function Calls**: Github
- **Imports Used**: None
- **Code Lines**: 2

---
#### fetch_pr_context (line 11)

**Signature:** `def fetch_pr_context(self, repo_name: str, pr_number: int):`

**Complete Code:**
```python
def fetch_pr_context(self, repo_name: str, pr_number: int):
        repo = self.github.get_repo(repo_name)
        pr = repo.get_pull(pr_number)
        return {
            "pr_info": {
                "title": pr.title,
                "description": pr.body,
                "author": pr.user.login,
                "state": pr.state,
                "created_at": pr.created_at.isoformat(),
                "base_branch": pr.base.ref,
                "head_branch": pr.head.ref,
            },
            "commits": self._get_pr_commits(pr),
            "all_comments": self._get_pr_comments(pr),
            # "review_threads": self._get_pr_review(pr)
        }
```

**📊 Semantic Analysis:**
- **Function Calls**: self.github.get_repo, repo.get_pull, pr.created_at.isoformat, self._get_pr_commits, self._get_pr_comments
- **Imports Used**: None
- **Code Lines**: 17

---
#### _get_pr_commits (line 29)

**Signature:** `def _get_pr_commits(self, pr) -> List[Dict]:`

**Complete Code:**
```python
def _get_pr_commits(self, pr) -> List[Dict]:
        commits = []
        for commit in pr.get_commits():
            commits.append(
                {
                    "sha": commit.sha,
                    "message": commit.commit.message,
                    "author": commit.commit.author.name,
                    "date": commit.commit.author.date.isoformat(),
                    "files_changed": [file.filename for file in commit.files],
                }
            )
        return commits
```

**📊 Semantic Analysis:**
- **Function Calls**: pr.get_commits, commits.append, commit.commit.author.date.isoformat
- **Imports Used**: None
- **Code Lines**: 13

---
#### _get_pr_comments (line 43)

**Signature:** `def _get_pr_comments(self, pr, bot_name: str = "My-Code-Comment-Bot") -> List[Dict]:`

**Complete Code:**
```python
def _get_pr_comments(self, pr, bot_name: str = "My-Code-Comment-Bot") -> List[Dict]:
        bot_comments = []
        for comment in pr.get_issue_comments():
            if comment.user.login.lower() == bot_name.lower():
                bot_comments.append(
                    {
                        "type": "issue_comment",
                        "comment": comment.body,
                        "created_at": comment.created_at.isoformat(),
                        "author": comment.user.login,
                    }
                )

        for comment in pr.get_review_comments():
            if comment.user.login.lower() == bot_name.lower():
                bot_comments.append(
                    {
                        "type": "review_comment",
                        "comment": comment.body,
                        "file": comment.path,
                        "line": comment.position,
                        "created_at": comment.created_at.isoformat(),
                        "author": comment.user.login,
                    }
                )
        return bot_comments
```

**📊 Semantic Analysis:**
- **Function Calls**: pr.get_issue_comments, comment.user.login.lower, bot_name.lower, bot_comments.append, comment.created_at.isoformat, pr.get_review_comments
- **Imports Used**: None
- **Code Lines**: 26

---
### Enhanced Function Dependencies (from Semantic Graph)

- **__init__()** → **Github()**
- **fetch_pr_context()** → **self.github.get_repo()**
- **fetch_pr_context()** → **repo.get_pull()**
- **fetch_pr_context()** → **pr.created_at.isoformat()**
- **fetch_pr_context()** → **self._get_pr_commits()**
- **fetch_pr_context()** → **self._get_pr_comments()**
- **_get_pr_commits()** → **pr.get_commits()**
- **_get_pr_commits()** → **commits.append()**
- **_get_pr_commits()** → **commit.commit.author.date.isoformat()**
- **_get_pr_comments()** → **pr.get_issue_comments()**
- **_get_pr_comments()** → **comment.user.login.lower()**
- **_get_pr_comments()** → **bot_name.lower()**
- **_get_pr_comments()** → **bot_comments.append()**
- **_get_pr_comments()** → **comment.created_at.isoformat()**
- **_get_pr_comments()** → **pr.get_review_comments()**

### Import Usage Analysis (from Semantic Graph)


### All Imports Used

- **typing** (from import, line 1)
- **gitub** (from import, line 3)
- **utis.config** (from import, line 4)


## Enhanced File Analysis: `backend/src/services/semantic_graph_builder.py`

### Analysis Summary
- **Analysis Method**: hybrid_detailed_semantic
- **Functions**: 15
- **Classes**: 2
- **Imports**: 2
- **Semantic Graph Nodes**: 52
- **Semantic Graph Edges**: 72

### Graph Statistics
- **Functions in Graph**: 15
- **Classes in Graph**: 2
- **Imports in Graph**: 4

### Classes

- **SemanticNode** (line 8)
- **SemanticGraphBuilder** (line 22)

### Functions with Complete Code & Semantic Analysis

#### _handle_js_ts_definitions (line 129)

**Signature:** `def _handle_js_ts_definitions(`

**Complete Code:**
```python
def _handle_js_ts_definitions(
        self, node, current_def: Optional[str]
    ) -> Optional[str]:
        """Handle JavaScript/TypeScript function and class definitions"""
        if node.type in ["function_declaration", "method_definition"]:
            name_node = node.child_by_field_name("name")
            if name_node:
                name = self._get_node_text(name_node)
                params_node = node.child_by_field_name("parameters")
                params = self._get_node_text(params_node) if params_node else ""

                def_id = self._add_definition(
                    name=name, node_type="function", ast_node=node, parameters=params
                )

                if current_def:
                    self._add_edge(current_def, def_id, "contains")

                return def_id

        elif node.type == "class_declaration":
            name_node = node.child_by_field_name("name")
            if name_node:
                name = self._get_node_text(name_node)

                class_id = self._add_definition(
                    name=name, node_type="class", ast_node=node
                )

                if current_def:
                    self._add_edge(current_def, class_id, "contains")

                return class_id

        return current_def
```

**📊 Semantic Analysis:**
- **Function Calls**: node.child_by_field_name, self._get_node_text, self._add_definition, self._add_edge
- **Imports Used**: None
- **Code Lines**: 35

---
#### _walk_ast (line 65)

**Signature:** `def _walk_ast(self, node, current_def: Optional[str] = None):`

**Complete Code:**
```python
def _walk_ast(self, node, current_def: Optional[str] = None):
        """Recursively walk AST nodes and build semantic relationships"""

        # Track current definition scope
        original_def = current_def

        # Handle definitions (functions, classes)
        if self.language == "python":
            current_def = self._handle_python_definitions(node, current_def)
        elif self.language in ["javascript", "typescript"]:
            current_def = self._handle_js_ts_definitions(node, current_def)
        elif self.language == "go":
            current_def = self._handle_go_definitions(node, current_def)

        # Handle imports
        self._handle_imports(node, current_def)

        # Handle function calls
        self._handle_function_calls(node, current_def)

        # Recursively process children
        for child in node.children:
            self._walk_ast(child, current_def)

        # Restore original scope
        current_def = original_def
```

**📊 Semantic Analysis:**
- **Function Calls**: self._handle_python_definitions, self._handle_js_ts_definitions, self._handle_go_definitions, self._handle_imports, self._handle_function_calls, self._walk_ast
- **Imports Used**: None
- **Code Lines**: 26

---
#### _handle_imports (line 185)

**Signature:** `def _handle_imports(self, node, current_def: Optional[str]):`

**Complete Code:**
```python
def _handle_imports(self, node, current_def: Optional[str]):
        """Handle import statements and create import nodes"""
        if self.language == "python":
            if node.type in ["import_statement", "import_from_statement"]:
                import_text = self._get_node_text(node).strip()
                import_id = f"{self.file_path}::import::{import_text}"

                self._add_node(
                    node_id=import_id,
                    name=import_text,
                    node_type="import",
                    line=node.start_point[0] + 1,
                    span=(node.start_point, node.end_point),
                    metadata={"code": import_text},
                )

                # Connect to containing function or file
                if current_def:
                    self._add_edge(current_def, import_id, "uses_import")
                else:
                    file_id = f"{self.file_path}::file"
                    self._add_edge(file_id, import_id, "imports")

        elif self.language in ["javascript", "typescript"]:
            if node.type == "import_statement":
                # Extract module name from import statement
                for child in node.children:
                    if child.type == "string":
                        module_name = self._get_node_text(child).strip("\"'")
                        import_id = f"{self.file_path}::import::{module_name}"

                        self._add_node(
                            node_id=import_id,
                            name=module_name,
                            node_type="import",
                            line=node.start_point[0] + 1,
                            span=(node.start_point, node.end_point),
                            metadata={"code": module_name},
                        )

                        if current_def:
                            self._add_edge(current_def, import_id, "uses_import")
                        else:
                            file_id = f"{self.file_path}::file"
                            self._add_edge(file_id, import_id, "imports")
                        break
```

**📊 Semantic Analysis:**
- **Function Calls**: self._get_node_text, self._add_node, self._add_edge
- **Imports Used**: None
- **Code Lines**: 46

---
#### _add_definition (line 278)

**Signature:** `def _add_definition(`

**Complete Code:**
```python
def _add_definition(
        self, name: str, node_type: str, ast_node, parameters: str = ""
    ) -> str:
        """Add a function or class definition to the graph"""
        def_id = f"{self.file_path}::{node_type}::{name}"

        self._add_node(
            node_id=def_id,
            name=name,
            node_type=node_type,
            line=ast_node.start_point[0] + 1,
            span=(ast_node.start_point, ast_node.end_point),
            code=self._get_node_text(ast_node),
            parameters=parameters,
        )

        return def_id
```

**📊 Semantic Analysis:**
- **Function Calls**: self._add_node, self._get_node_text
- **Imports Used**: None
- **Code Lines**: 17

---
#### _add_edge (line 320)

**Signature:** `def _add_edge(self, from_id: str, to_id: str, edge_type: str):`

**Complete Code:**
```python
def _add_edge(self, from_id: str, to_id: str, edge_type: str):
        """Add a typed edge to the semantic graph"""
        self.graph.add_edge(from_id, to_id, type=edge_type)
```

**📊 Semantic Analysis:**
- **Function Calls**: self.graph.add_edge
- **Imports Used**: None
- **Code Lines**: 3

---
#### get_function_dependencies (line 333)

**Signature:** `def get_function_dependencies(self) -> Dict[str, List[str]]:`

**Complete Code:**
```python
def get_function_dependencies(self) -> Dict[str, List[str]]:
        """Extract function dependencies from the semantic graph"""
        dependencies = {}

        for node_id, node_data in self.graph.nodes(data=True):
            if node_data["type"] == "function":
                func_name = node_data["name"]
                dependencies[func_name] = []

                # Find all outgoing edges (calls this function makes)
                for _, target_id, edge_data in self.graph.out_edges(node_id, data=True):
                    if edge_data["type"] == "calls":
                        target_node = self.graph.nodes[target_id]
                        dependencies[func_name].append(target_node["name"])

        return dependencies
```

**📊 Semantic Analysis:**
- **Function Calls**: self.graph.nodes, self.graph.out_edges, dependencies[func_name].append
- **Imports Used**: None
- **Code Lines**: 16

---
#### __init__ (line 28)

**Signature:** `def __init__(self, language: str):`

**Complete Code:**
```python
def __init__(self, language: str):
        self.language = language
        self.graph = nx.DiGraph()
        # Import here to avoid circular dependency
        from .ast_parser import MultiLanguageAnalyzer

        self.ast_parser = MultiLanguageAnalyzer(language)
        self.current_definitions: List[str] = []  # Track current scope (function/class)
        self.file_path: str = ""
```

**📊 Semantic Analysis:**
- **Function Calls**: nx.DiGraph, MultiLanguageAnalyzer
- **Imports Used**: from .ast_parser import MultiLanguageAnalyzer
- **Code Lines**: 9

---
#### _handle_python_definitions (line 92)

**Signature:** `def _handle_python_definitions(`

**Complete Code:**
```python
def _handle_python_definitions(
        self, node, current_def: Optional[str]
    ) -> Optional[str]:
        """Handle Python function and class definitions"""
        if node.type == "function_definition":
            name_node = node.child_by_field_name("name")
            if name_node:
                name = self._get_node_text(name_node)
                params_node = node.child_by_field_name("parameters")
                params = self._get_node_text(params_node) if params_node else ""

                def_id = self._add_definition(
                    name=name, node_type="function", ast_node=node, parameters=params
                )

                # Add contains relationship if we're inside another definition
                if current_def:
                    self._add_edge(current_def, def_id, "contains")

                return def_id

        elif node.type == "class_definition":
            name_node = node.child_by_field_name("name")
            if name_node:
                name = self._get_node_text(name_node)

                class_id = self._add_definition(
                    name=name, node_type="class", ast_node=node
                )

                if current_def:
                    self._add_edge(current_def, class_id, "contains")

                return class_id

        return current_def
```

**📊 Semantic Analysis:**
- **Function Calls**: node.child_by_field_name, self._get_node_text, self._add_definition, self._add_edge
- **Imports Used**: None
- **Code Lines**: 36

---
#### build_semantic_graph (line 38)

**Signature:** `def build_semantic_graph(`

**Complete Code:**
```python
def build_semantic_graph(
        self, tree, source_code: bytes, file_path: str
    ) -> nx.DiGraph:
        """
        Build a semantic graph from AST with:
        - Nodes: functions, classes, imports, call targets
        - Edges: defines, calls, imports, contains relationships
        """
        self.graph = nx.DiGraph()
        self.file_path = file_path
        self.source_code = source_code

        # Add file anchor node
        file_node_id = f"{file_path}::file"
        self._add_node(
            node_id=file_node_id,
            name=file_path,
            node_type="file",
            line=1,
            span=((0, 0), (0,)),
        )

        # Walk the AST and build semantic relationships
        self._walk_ast(tree.root_node)

        return self.graph
```

**📊 Semantic Analysis:**
- **Function Calls**: nx.DiGraph, self._add_node, self._walk_ast
- **Imports Used**: None
- **Code Lines**: 26

---
#### _handle_go_definitions (line 165)

**Signature:** `def _handle_go_definitions(self, node, current_def: Optional[str]) -> Optional[str]:`

**Complete Code:**
```python
def _handle_go_definitions(self, node, current_def: Optional[str]) -> Optional[str]:
        """Handle Go function and type definitions"""
        if node.type in ["function_declaration", "method_declaration"]:
            name_node = node.child_by_field_name("name")
            if name_node:
                name = self._get_node_text(name_node)
                params_node = node.child_by_field_name("parameters")
                params = self._get_node_text(params_node) if params_node else ""

                def_id = self._add_definition(
                    name=name, node_type="function", ast_node=node, parameters=params
                )

                if current_def:
                    self._add_edge(current_def, def_id, "contains")

                return def_id

        return current_def
```

**📊 Semantic Analysis:**
- **Function Calls**: node.child_by_field_name, self._get_node_text, self._add_definition, self._add_edge
- **Imports Used**: None
- **Code Lines**: 19

---
#### _handle_function_calls (line 232)

**Signature:** `def _handle_function_calls(self, node, current_def: Optional[str]):`

**Complete Code:**
```python
def _handle_function_calls(self, node, current_def: Optional[str]):
        """Handle function calls and create call relationships"""
        if self.language == "python" and node.type == "call":
            func_node = None
            if node.child_by_field_name("function"):
                func_node = node.child_by_field_name("function")
            elif len(node.children) > 0:
                func_node = node.children[0]

            if func_node and current_def:
                called_name = self._get_node_text(func_node).split("(")[0].strip()
                if called_name:
                    call_id = f"{self.file_path}::call::{called_name}"

                    self._add_node(
                        node_id=call_id,
                        name=called_name,
                        node_type="call_target",
                        line=node.start_point[0] + 1,
                        span=(node.start_point, node.end_point),
                    )

                    self._add_edge(current_def, call_id, "calls")

        elif (
            self.language in ["javascript", "typescript"]
            and node.type == "call_expression"
        ):
            # Find the function being called
            for child in node.children:
                if child.type in ["identifier", "member_expression"]:
                    called_name = self._get_node_text(child).split("(")[0].strip()
                    if called_name and current_def:
                        call_id = f"{self.file_path}::call::{called_name}"

                        self._add_node(
                            node_id=call_id,
                            name=called_name,
                            node_type="call_target",
                            line=node.start_point[0] + 1,
                            span=(node.start_point, node.end_point),
                        )

                        self._add_edge(current_def, call_id, "calls")
                    break
```

**📊 Semantic Analysis:**
- **Function Calls**: node.child_by_field_name, len, self._get_node_text, self._add_node, self._add_edge
- **Imports Used**: None
- **Code Lines**: 45

---
#### _add_node (line 296)

**Signature:** `def _add_node(`

**Complete Code:**
```python
def _add_node(
        self,
        node_id: str,
        name: str,
        node_type: str,
        line: int,
        span: Tuple[Tuple[int, int], Tuple[int, int]],
        code: Optional[str] = None,
        parameters: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """Add a node to the semantic graph"""
        self.graph.add_node(
            node_id,
            name=name,
            type=node_type,
            file_path=self.file_path,
            line=line,
            span=span,
            code=code,
            parameters=parameters,
            metadata=metadata or {},
        )
```

**📊 Semantic Analysis:**
- **Function Calls**: self.graph.add_node
- **Imports Used**: None
- **Code Lines**: 23

---
#### _get_node_text (line 324)

**Signature:** `def _get_node_text(self, node) -> str:`

**Complete Code:**
```python
def _get_node_text(self, node) -> str:
        """Extract text from an AST node"""
        if node is None:
            return ""
        try:
            return self.source_code[node.start_byte : node.end_byte].decode("utf-8")
        except Exception:
            return ""
```

**📊 Semantic Analysis:**
- **Function Calls**: self.source_code[node.start_byte : node.end_byte].decode
- **Imports Used**: None
- **Code Lines**: 8

---
#### get_import_usage (line 350)

**Signature:** `def get_import_usage(self) -> Dict[str, List[str]]:`

**Complete Code:**
```python
def get_import_usage(self) -> Dict[str, List[str]]:
        """Extract which functions use which imports"""
        import_usage = {}

        for node_id, node_data in self.graph.nodes(data=True):
            if node_data["type"] == "function":
                func_name = node_data["name"]
                import_usage[func_name] = []

                # Find all imports used by this function
                for _, target_id, edge_data in self.graph.out_edges(node_id, data=True):
                    if edge_data["type"] == "uses_import":
                        import_node = self.graph.nodes[target_id]
                        import_usage[func_name].append(import_node["name"])

        return import_usage
```

**📊 Semantic Analysis:**
- **Function Calls**: self.graph.nodes, self.graph.out_edges, import_usage[func_name].append
- **Imports Used**: None
- **Code Lines**: 16

---
#### to_analysis_dict (line 367)

**Signature:** `def to_analysis_dict(self) -> Dict[str, Any]:`

**Complete Code:**
```python
def to_analysis_dict(self) -> Dict[str, Any]:
        """Convert semantic graph to analysis dictionary for context building"""
        functions = []
        classes = []
        imports = []

        for node_id, node_data in self.graph.nodes(data=True):
            if node_data["type"] == "function":
                functions.append(
                    {
                        "name": node_data["name"],
                        "line": node_data["line"],
                        "parameters": node_data.get("parameters", ""),
                        "code": node_data.get("code", ""),
                        "span": node_data["span"],
                    }
                )
            elif node_data["type"] == "class":
                classes.append(
                    {
                        "name": node_data["name"],
                        "line": node_data["line"],
                        "span": node_data["span"],
                    }
                )
            elif node_data["type"] == "import":
                imports.append(
                    {
                        "module": node_data["name"],
                        "line": node_data["line"],
                        "type": "direct",
                    }
                )

        return {
            "functions": functions,
            "classes": classes,
            "imports": imports,
            "dependencies": self.get_function_dependencies(),
            "import_usage": self.get_import_usage(),
            "graph_stats": {
                "nodes": self.graph.number_of_nodes(),
                "edges": self.graph.number_of_edges(),
                "functions": len(functions),
                "classes": len(classes),
                "imports": len(imports),
            },
        }
```

**📊 Semantic Analysis:**
- **Function Calls**: self.graph.nodes, functions.append, node_data.get, classes.append, imports.append, self.get_function_dependencies, self.get_import_usage, self.graph.number_of_nodes, self.graph.number_of_edges, len
- **Imports Used**: None
- **Code Lines**: 48

---
### Enhanced Function Dependencies (from Semantic Graph)

- **__init__()** → **nx.DiGraph()**
- **__init__()** → **MultiLanguageAnalyzer()**
- **build_semantic_graph()** → **nx.DiGraph()**
- **build_semantic_graph()** → **self._add_node()**
- **build_semantic_graph()** → **self._walk_ast()**
- **_walk_ast()** → **self._handle_python_definitions()**
- **_walk_ast()** → **self._handle_js_ts_definitions()**
- **_walk_ast()** → **self._handle_go_definitions()**
- **_walk_ast()** → **self._handle_imports()**
- **_walk_ast()** → **self._handle_function_calls()**
- **_walk_ast()** → **self._walk_ast()**
- **_handle_python_definitions()** → **node.child_by_field_name()**
- **_handle_python_definitions()** → **self._get_node_text()**
- **_handle_python_definitions()** → **self._add_definition()**
- **_handle_python_definitions()** → **self._add_edge()**
- **_handle_js_ts_definitions()** → **node.child_by_field_name()**
- **_handle_js_ts_definitions()** → **self._get_node_text()**
- **_handle_js_ts_definitions()** → **self._add_definition()**
- **_handle_js_ts_definitions()** → **self._add_edge()**
- **_handle_go_definitions()** → **node.child_by_field_name()**
- **_handle_go_definitions()** → **self._get_node_text()**
- **_handle_go_definitions()** → **self._add_definition()**
- **_handle_go_definitions()** → **self._add_edge()**
- **_handle_imports()** → **self._get_node_text()**
- **_handle_imports()** → **self._add_node()**
- **_handle_imports()** → **self._add_edge()**
- **_handle_function_calls()** → **node.child_by_field_name()**
- **_handle_function_calls()** → **len()**
- **_handle_function_calls()** → **self._get_node_text()**
- **_handle_function_calls()** → **self._add_node()**
- **_handle_function_calls()** → **self._add_edge()**
- **_add_definition()** → **self._add_node()**
- **_add_definition()** → **self._get_node_text()**
- **_add_node()** → **self.graph.add_node()**
- **_add_edge()** → **self.graph.add_edge()**
- **_get_node_text()** → **self.source_code[node.start_byte : node.end_byte].decode()**
- **get_function_dependencies()** → **self.graph.nodes()**
- **get_function_dependencies()** → **self.graph.out_edges()**
- **get_function_dependencies()** → **dependencies[func_name].append()**
- **get_import_usage()** → **self.graph.nodes()**
- **get_import_usage()** → **self.graph.out_edges()**
- **get_import_usage()** → **import_usage[func_name].append()**
- **to_analysis_dict()** → **self.graph.nodes()**
- **to_analysis_dict()** → **functions.append()**
- **to_analysis_dict()** → **node_data.get()**
- **to_analysis_dict()** → **classes.append()**
- **to_analysis_dict()** → **imports.append()**
- **to_analysis_dict()** → **self.get_function_dependencies()**
- **to_analysis_dict()** → **self.get_import_usage()**
- **to_analysis_dict()** → **self.graph.number_of_nodes()**
- **to_analysis_dict()** → **self.graph.number_of_edges()**
- **to_analysis_dict()** → **len()**

### Import Usage Analysis (from Semantic Graph)

- **__init__()** uses: from .ast_parser import MultiLanguageAnalyzer

### All Imports Used

- **dataclasses** (from import, line 1)
- **typing** (from import, line 2)


## Enhanced File Analysis: `backend/src/webhook/github_webhook.py`

### Analysis Summary
- **Analysis Method**: hybrid_detailed_semantic
- **Functions**: 2
- **Classes**: 0
- **Imports**: 13
- **Semantic Graph Nodes**: 45
- **Semantic Graph Edges**: 42

### Graph Statistics
- **Functions in Graph**: 2
- **Classes in Graph**: 0
- **Imports in Graph**: 11

### Functions with Complete Code & Semantic Analysis

#### verify_signature (line 14)

**Signature:** `def verify_signature(payload: Any, signature: str):`

**Complete Code:**
```python
def verify_signature(payload: Any, signature: str):
    mac = hmac.new(
        settings.github_webhook_secret.encode(),
        msg=payload,
        digestmod=hashlib.sha256
    )
    return hmac.compare_digest(
        f"sha256={mac.hexdigest()}",
        signature
    )
```

**📊 Semantic Analysis:**
- **Function Calls**: hmac.new, settings.github_webhook_secret.encode, hmac.compare_digest, mac.hexdigest
- **Imports Used**: None
- **Code Lines**: 10

---
#### github_webhook (line 26)

**Signature:** `async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_hub_signature_256: Optional[str] = Header(None, alias="X-Hub-Signature-256"), x_github_event: Optional[str] = Header(None, alias="X-GitHub-Event")):`

**Complete Code:**
```python
async def github_webhook(request: Request, background_tasks: BackgroundTasks, x_hub_signature_256: Optional[str] = Header(None, alias="X-Hub-Signature-256"), x_github_event: Optional[str] = Header(None, alias="X-GitHub-Event")):
    payload = await request.body()
    if not verify_signature(payload, x_hub_signature_256):
        raise HTTPException(status_code=401, detail="Invalid signature")
    payload = json.loads(payload.decode('utf-8'))
    if x_github_event != "pull_request":
        return {"status": "skipped", "event": x_github_event}
    action = payload.get("action", "")
    pr = payload.get("pull_request", {})
    repo = payload.get("repository", {})
    installation_id = payload.get("installation", {}).get("id")
    pr_number = pr.get("number")
    pr_title = pr.get("title", "")
    repo_url = repo.get("clone_url", "")
    repo_full_name = repo.get("full_name", "")
    base_branch = pr.get("base", {}).get("ref", "main")
    head_branch = pr.get("head", {}).get("ref", "")
    print(f"Action: {action}")
    print(f"PR: {pr_number}: {pr_title}")
    print(f"Base branch: {base_branch}, Head branch: {head_branch}")
    print(f"Repo: {repo_url}")
    try:
        print("Cloning the repository and fetching branches..")
        repo_path = repo_manager.clone_and_setup_repo(
            repo_url = repo_url,
            pr_number = pr_number,
            head_branch = head_branch,
            base_branch = base_branch
        )
        print(f"Repository cloned to: {repo_path}")
        print(f"Now getting diffs..")
        diff_data = repo_manager.get_diff(
            repo_path = repo_path,
            base_branch = base_branch,
            head_branch = head_branch
        )
        print(f"Diff generated successfully: {diff_data}")
        print(f"Total files changed: {len(diff_data['diff_files'])}")

        # Log raw diff data for inspection
        print("=" * 50)
        print("🔄 RAW DIFF DATA FETCHED:")
        print("=" * 50)
        print(f"PR Title: {diff_data.get('pr_title', 'N/A')}")
        print(f"PR Description: {diff_data.get('pr_description', 'N/A')[:100]}...")
        print(f"Full diff length: {len(diff_data.get('full_diff', '')):,} characters")
        print(f"Changed files: {diff_data.get('diff_files', [])}")
        print("=" * 50)
        # Fetch PR history first
        print("Fetching PR history...")
        history_fetcher = HistoryFetcher()
        pr_history = history_fetcher.fetch_pr_context(repo_full_name, pr_number)

        # Log raw PR history for inspection
        print("=" * 50)
        print("📚 RAW PR HISTORY FETCHED:")
        print("=" * 50)
        print(f"Commits: {len(pr_history.get('commits', []))}")
        print(f"Comments: {len(pr_history.get('all_comments', []))}")
        print("Sample commit:", pr_history.get('commits', [{}])[0] if pr_history.get('commits') else {})
        print("=" * 50)

        print("Building enhanced AI context with AST parser...")

        # Add PR metadata to diff_data
        diff_data['pr_title'] = pr_title
        diff_data['pr_description'] = pr.get('body', '')

        # Initialize enhanced context builder
        context_builder = EnhancedContextBuilder()

        # Build comprehensive context (diff + history + AST analysis)
        comprehensive_context = context_builder.build_comprehensive_ai_context(
            diff_data=diff_data,
            pr_history=pr_history,
            repo_path=repo_path
        )

        print(f"Generated enhanced context length: {len(comprehensive_context)} characters")

        # Save complete context to file for inspection
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        context_file = f"ai_context_{pr_number}_{timestamp}.md"

        with open(context_file, 'w', encoding='utf-8') as f:
            f.write(comprehensive_context)
        print(f"💾 Complete context saved to: {context_file}")

        # Log complete context for inspection (optional - comment out if too verbose)
        print("=" * 80)
        print("📋 COMPLETE ENHANCED CONTEXT SENT TO AI:")
        print("=" * 80)
        print(comprehensive_context)
        print("=" * 80)
        print(f"📏 Context length: {len(comprehensive_context):,} characters")
        print("=" * 80)

        print(f"Getting AI to review with enhanced context...")
        ai_review = review_code(
            diff = diff_data['full_diff'],
            pr_title = pr_title,
            context = comprehensive_context
        )
        print(f"AI review completed: {ai_review}")
        github_bot = GitHubBot(installation_id=installation_id)
        print(f"Starting to send the ai review to the bot..: {installation_id}")
        comment = github_bot.post_review_comment(
            repo_full_name = repo_full_name,
            pr_number = pr_number,
            ai_review = ai_review
        )
        if comment:
            print(f"Successfully commented")
        else:
            print(f"Failed to comment")
        # print(f"Now cleaning up..")
        # repo_manager.clean_up(repo_path)
        # print("Completed cleanup")
        return {
            "status": "success",
            "prn_number": pr_number,
            "files_changed": len(diff_data['diff_files']),
            "changed_files": diff_data['diff_files']
        }
    except Exception as e:
        print(f"ERROR in webhook: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
```

**📊 Semantic Analysis:**
- **Function Calls**: Header, request.body, verify_signature, HTTPException, json.loads, payload.decode, payload.get, pr.get, repo.get, print, repo_manager.clone_and_setup_repo, repo_manager.get_diff, len, diff_data.get, HistoryFetcher, history_fetcher.fetch_pr_context, pr_history.get, EnhancedContextBuilder, context_builder.build_comprehensive_ai_context, datetime.datetime.now, open, f.write, review_code, GitHubBot, github_bot.post_review_comment, traceback.print_exc, str
- **Imports Used**: import datetime, import traceback
- **Code Lines**: 130

---
### Enhanced Function Dependencies (from Semantic Graph)

- **verify_signature()** → **hmac.new()**
- **verify_signature()** → **settings.github_webhook_secret.encode()**
- **verify_signature()** → **hmac.compare_digest()**
- **verify_signature()** → **mac.hexdigest()**
- **github_webhook()** → **Header()**
- **github_webhook()** → **request.body()**
- **github_webhook()** → **verify_signature()**
- **github_webhook()** → **HTTPException()**
- **github_webhook()** → **json.loads()**
- **github_webhook()** → **payload.decode()**
- **github_webhook()** → **payload.get()**
- **github_webhook()** → **pr.get()**
- **github_webhook()** → **repo.get()**
- **github_webhook()** → **print()**
- **github_webhook()** → **repo_manager.clone_and_setup_repo()**
- **github_webhook()** → **repo_manager.get_diff()**
- **github_webhook()** → **len()**
- **github_webhook()** → **diff_data.get()**
- **github_webhook()** → **HistoryFetcher()**
- **github_webhook()** → **history_fetcher.fetch_pr_context()**
- **github_webhook()** → **pr_history.get()**
- **github_webhook()** → **EnhancedContextBuilder()**
- **github_webhook()** → **context_builder.build_comprehensive_ai_context()**
- **github_webhook()** → **datetime.datetime.now()**
- **github_webhook()** → **open()**
- **github_webhook()** → **f.write()**
- **github_webhook()** → **review_code()**
- **github_webhook()** → **GitHubBot()**
- **github_webhook()** → **github_bot.post_review_comment()**
- **github_webhook()** → **traceback.print_exc()**
- **github_webhook()** → **str()**

### Import Usage Analysis (from Semantic Graph)

- **github_webhook()** uses: import datetime, import traceback

### All Imports Used

- **utils.config** (from import, line 4)
- **typing** (from import, line 3)
- **git_ops.repo_manager** (from import, line 5)
- **utils.github_bot** (from import, line 7)
- **services.enhanced_context_builder** (from import, line 9)
- **services.history_fetcher** (from import, line 8)
- **ai.code_reviewer** (from import, line 6)
- **fastapi** (from import, line 1)
- **hmac** (direct import, line 2)
- **datetime** (direct import, line 107)
- **traceback** (direct import, line 153)
- **hashlib** (direct import, line 2)
- **json** (direct import, line 2)




## AI Instructions

Please provide a comprehensive code review that considers:

1. **Code Quality**: Best practices, patterns, potential improvements
2. **Security**: Any security vulnerabilities or concerns
3. **Performance**: Performance implications and optimizations
4. **Dependencies**: Impact of new imports and function dependencies
5. **Maintainability**: Code readability and future maintenance
6. **Testing**: Testability and testing suggestions

Focus on the changed functions and their dependencies. Consider the PR history to avoid repeating previous suggestions.

---
*Context generated by enhanced AST parser + PR history analysis*
