# Pull Request Analysis

## PR Information
- **Title**: New PR raised
- **Description**: None
- **Files Changed**: 2
- **Base Branch**: main
- **Head Branch**: feature


## PR History Context

### PR Details
- **Author**: kr1shna-exe
- **State**: open
- **Created**: 2025-10-08T12:40:39+00:00
- **Base Branch**: main

### Recent Commits

**ea48b2f0dfed9af0b48e479f7a504bc914cc2a6b** - jkchinnu444444@gmail.com (2025-10-08T12:40:09+00:00)
- **Files**: backend/src/services/enhanced_context_builder.py
- **Message**: test

**35e81d7241f8762d214edb777d51159c1e20b95c** - jkchinnu444444@gmail.com (2025-10-08T16:25:11+00:00)
- **Files**: backend/src/services/ast_parser.py
- **Message**: testing

**5b946acf2a99cc152391a896595e4e6b57993148** - jkchinnu444444@gmail.com (2025-10-08T16:33:14+00:00)
- **Files**: backend/src/services/ast_parser.py
- **Message**: testing


## Code Changes (Diff)

### Complete Diff

```diff
diff --git a/backend/src/services/ast_parser.py b/backend/src/services/ast_parser.py
index 219a37f..e08e3f1 100644
--- a/backend/src/services/ast_parser.py
+++ b/backend/src/services/ast_parser.py
@@ -1,12 +1,15 @@
+from typing import Any, Dict, List, Optional
+
+import as
 import tree_sitter_go as tsgo
 import tree_sitter_javascript as tsjs
 import tree_sitter_python as tspython
-import tree_sitter_rust as tsrust
+import tree_sitter_rust
 import tree_sitter_typescript as tsts
 from tree_sitter import Language, Parser, Query, QueryCursor
-import re
-from typing import Dict, List, Optional, Any
-from .semantic_graph_builder import SemanticGraphBuilder
+
+from .semanticgraph_builder import SemanticGraphBuilder
+
 
 class MultiLanguageAnalyzer:
     LANGUAGES = {
@@ -75,7 +78,7 @@ class MultiLanguageAnalyzer:
             captures = cursor.captures(tree.root_node)
 
             classes = []
-            for capture_name, nodes in captures.items():
+            for captur_iname, nodes in captures.items():
                 if "class.name" in capture_name:
                     for node in nodes:
                         classes.append(
@@ -327,7 +330,11 @@ class MultiLanguageAnalyzer:
         For enhanced analysis with AST-accurate relationships, use extract_semantic_analysis().
         """
         if not code:
-            return {"internal_calls": [], "external_imports": [], "function_dependencies": {}}
+            return {
+                "internal_calls": [],
+                "external_imports": [],
+                "function_dependencies": {},
+            }
 
         functions = self.extract_functions(tree, code)
         imports = self.extract_imports(tree)
@@ -345,11 +352,13 @@ class MultiLanguageAnalyzer:
                 if other_func["name"] != func_name:
                     pattern = r"\b" + re.escape(other_func["name"]) + r"\s*\("
                     if re.search(pattern, func_code):
-                        internal_calls.append({
-                            "caller": func_name,
-                            "callee": other_func["name"],
-                            "line": func["line"],
-                        })
+                        internal_calls.append(
+                            {
+                                "caller": func_name,
+                                "callee": other_func["name"],
+                                "line": func["line"],
+                            }
+                        )
 
             # Basic import usage detection (text-based)
             for imp in imports:
@@ -370,7 +379,9 @@ class MultiLanguageAnalyzer:
             },
         }
 
-    def extract_semantic_analysis(self, tree, source_code: str, file_path: str) -> Dict[str, Any]:
+    def extract_semantic_analysis(
+        self, tree, source_code: str, file_path: str
+    ) -> Dict[str, Any]:
         """
         Enhanced analysis using semantic graphs (combining our approach with friend's approach)
         """
@@ -380,9 +391,7 @@ class MultiLanguageAnalyzer:
 
             # Build semantic graph
             semantic_graph = graph_builder.build_semantic_graph(
-                tree,
-                source_code.encode('utf-8'),
-                file_path
+                tree, source_code.encode("utf-8"), file_path
             )
 
             # Get analysis from semantic graph
@@ -399,32 +408,33 @@ class MultiLanguageAnalyzer:
                 "detailed_functions": our_functions,
                 "detailed_imports": our_imports,
                 "detailed_classes": our_classes,
-
                 # Friend's semantic graph analysis (relationships)
                 "semantic_functions": semantic_analysis["functions"],
                 "semantic_imports": semantic_analysis["imports"],
                 "semantic_classes": semantic_analysis["classes"],
                 "function_dependencies": semantic_analysis["dependencies"],
                 "import_usage": semantic_analysis["import_usage"],
-
                 # Graph statistics
                 "graph_stats": semantic_analysis["graph_stats"],
-
                 # Metadata
                 "file_path": file_path,
                 "language": self.lang_name,
-                "analysis_method": "hybrid_detailed_semantic"
+                "analysis_method": "hybrid_detailed_semantic",
             }
 
             return enhanced_analysis
 
         except Exception as e:
-            print(f"Warning: Semantic analysis failed, falling back to basic extraction: {e}")
+            print(
+                f"Warning: Semantic analysis failed, falling back to basic extraction: {e}"
+            )
             # Fallback to our existing methods
             return {
                 "detailed_functions": self.extract_functions(tree, source_code),
                 "detailed_imports": self.extract_imports(tree),
                 "detailed_classes": self.extract_classes(tree),
-                "function_dependencies": self.extract_dependencies(tree, source_code)["function_dependencies"],
-                "analysis_method": "fallback_basic"
+                "function_dependencies": self.extract_dependencies(tree, source_code)[
+                    "function_dependencies"
+                ],
+                "analysis_method": "fallback_basic",
             }
diff --git a/backend/src/services/enhanced_context_builder.py b/backend/src/services/enhanced_context_builder.py
index f8f1b9d..2c2c42f 100644
--- a/backend/src/services/enhanced_context_builder.py
+++ b/backend/src/services/enhanced_context_builder.py
@@ -1,8 +1,5 @@
-import json
 from datetime import datetime
-from pathlib import Path
-from typing import Dict, List, Optional
-from .ast_parser import MultiLanguageAnalyzer
+
 
 class EnhancedContextBuilder:
     """
@@ -16,14 +13,14 @@ class EnhancedContextBuilder:
     def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
         """
         Creating markdown context from AST parser for a specific file
-        """        
+        """
         language = self._detect_language(file_path)
         if not language:
             return ""
 
         try:
             # Initializing AST parser for this language
-            ast_parser = MultiLanguageAnalyzer(language)
+            ast_parser = MultiLanguageAnalyzer(languag)
 
             # Read and analyze the file
             full_path = Path(repo_path) / file_path
@@ -37,7 +34,9 @@ class EnhancedContextBuilder:
             tree = ast_parser.parse_code(code)
 
             # Use enhanced semantic analysis (combining both approaches)
-            semantic_analysis = ast_parser.extract_semantic_analysis(tree, code, file_path)
+            semantic_analysis = ast_parser.extract_semantic_analysis(
+                tree, code, file_path
+            )
 
             # For backward compatibility, extract individual components
             functions = semantic_analysis.get("detailed_functions", [])
@@ -46,16 +45,14 @@ class EnhancedContextBuilder:
             classes = semantic_analysis.get("detailed_classes", [])
 
             # Convert to enhanced markdown with semantic information
-            markdown = self._convert_to_enhanced_markdown(
-                file_path, semantic_analysis
-            )
+            markdown = self._convert_to_enhanced_markdown(file_path, semantic_analysis)
 
             return markdown
 
         except Exception as e:
             return f"AST analysis failed for {file_path}: {str(e)}"
 
-    def _detect_language(self, file_path: str) -> Optional[str]:
+    def _detect_language(self, file_path: str) -> [str]:
         """Detect programming language from file extension"""
         ext = Path(file_path).suffix.lower()
         language_map = {
@@ -67,8 +64,9 @@ class EnhancedContextBuilder:
         }
         return language_map.get(ext)
 
-    
-    def _convert_to_enhanced_markdown(self, file_path: str, semantic_analysis: Dict) -> str:
+    def _convert_to_enhanced_markdown(
+        self, file_path: str, semantic_analysis: Dict
+    ) -> str:
         """Convert enhanced semantic analysis to markdown format with graph information"""
 
         # Extract components from semantic analysis
@@ -80,9 +78,6 @@ class EnhancedContextBuilder:
         graph_stats = semantic_analysis.get("graph_stats", {})
         analysis_method = semantic_analysis.get("analysis_method", "unknown")
 
-        markdown = f"""## Enhanced File Analysis: `{file_path}`
-
-### Analysis Summary
 - **Analysis Method**: {analysis_method}
 - **Functions**: {len(detailed_functions)}
 - **Classes**: {len(detailed_classes)}
@@ -106,8 +101,8 @@ class EnhancedContextBuilder:
         if detailed_functions:
             markdown += "### Functions with Complete Code & Semantic Analysis\n\n"
 
-            for func in detailed_functions:
-                func_name = func["name"]
+            for func in detailed_function:
+                func_name  func["function_name"]
 
                 # Enhanced import usage from semantic analysis
                 func_imports = import_usage.get(func_name, [])
@@ -273,7 +268,7 @@ Focus on the changed functions and their dependencies. Consider the PR history t
 
 """
 
-        # Bot comments 
+        # Bot comments
         bot_comments = pr_history.get("all_comments", [])
         if bot_comments:
             context += "### Previous AI Suggestions\n\n"
@@ -333,7 +328,11 @@ Focus on the changed functions and their dependencies. Consider the PR history t
 
         for file_path in files_to_analyze:
             ast_context = self.create_ast_markdown_context(file_path, repo_path)
-            if ast_context and not ast_context.startswith("File not found") and not ast_context.startswith("AST analysis failed"):
+            if (
+                ast_context
+                and not ast_context.startswith("File not found")
+                and not ast_context.startswith("AST analysis failed")
+            ):
                 context += ast_context + "\n"
                 analyzed_files += 1
 
@@ -341,4 +340,3 @@ Focus on the changed functions and their dependencies. Consider the PR history t
             context += "*No supported files found for AST analysis*\n\n"
 
         return context
-
```

### Changed Files

- `backend/src/services/ast_parser.py`
- `backend/src/services/enhanced_context_builder.py`


## Enhanced Code Analysis

## Enhanced File Analysis: `backend/src/services/ast_parser.py`

### Analysis Summary
- **Analysis Method**: ast_only
- **Functions**: 7
- **Classes**: 1
- **Imports**: 3
- **Semantic Graph Nodes**: 0
- **Semantic Graph Edges**: 0

### Classes

- **MultiLanguageAnalyzer** (line 14)

### Functions with Complete Code & Semantic Analysis

#### extract_imports (line 259)

**Signature:** `def extract_imports(self, tree):`

**Complete Code:**
```python
def extract_imports(self, tree):
        """Extracting import statements from AST"""

        queries = {
            "python": """
                (import_statement
                    name: (dotted_name) @import.module
                )
                (import_from_statement
                    module_name: (dotted_name) @import.from
                )
            """,
            "javascript": """
                (import_statement
                    source: (string) @import.module
                )
                (require
                    arguments: (arguments (string) @import.module)
                )
            """,
            "typescript": """
                (import_statement
                    source: (string) @import.module
                )
                (import_statement
                    module: (identifier) @import.module
                )
            """,
            "go": """
                (import_spec
                    path: (interpreted_string_literal) @import.module
                )
            """,
            "rust": """
                (use_declaration
                    argument: (use_path) @import.module
                )
            """,
        }

        query_str = queries.get(self.lang_name, "")
        if not query_str:
            return []

        try:
            query = Query(self.language, query_str)
            query_cursor = QueryCursor(query)
            captures = query_cursor.captures(tree.root_node)

            imports = []
            for capture_name, nodes in captures.items():
                for node in nodes:
                    module_name = node.text.decode("utf8").strip("\"'")
                    imports.append(
                        {
                            "module": module_name,
                            "line": node.start_point[0] + 1,
                            "type": "from" if "from" in capture_name else "direct",
                        }
                    )
            return imports
        except Exception as e:
            print(f"Warning: Import extraction failed for {self.lang_name}: {e}")
            return []
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 64

---
#### extract_classes (line 35)

**Signature:** `def extract_classes(self, tree):`

**Complete Code:**
```python
def extract_classes(self, tree):
        """Extracting class definitions from AST"""

        queries = {
            "python": """
                (class_definition
                    name: (identifier) @class.name
                )
            """,
            "javascript": """
                (class_declaration
                    name: (identifier) @class.name
                )
            """,
            "typescript": """
                (class_declaration
                    name: (identifier) @class.name
                )
                (interface_declaration
                    name: (type_identifier) @class.name
                )
            """,
            "go": "",
            "rust": """
                (struct_item
                    name: (type_identifier) @class.name
                )
                (impl_item
                    type: (type_identifier) @class.name
                )
                (enum_item
                    name: (type_identifier) @class.name
                )
            """,
        }

        query_str = queries.get(self.lang_name, "")
        if not query_str:
            return []

        try:
            query = Query(self.language, query_str)
            cursor = QueryCursor(query)
            captures = cursor.captures(tree.root_node)

            classes = []
            for captur_iname, nodes in captures.items():
                if "class.name" in capture_name:
                    for node in nodes:
                        classes.append(
                            {
                                "name": node.text.decode("utf8"),
                                "line": node.start_point[0] + 1,
                            }
                        )
            return classes
        except Exception as e:
            print(f"Warning: Class extraction failed for {self.lang_name}: {e}")
            return []
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 59

---
#### __init__ (line 23)

**Signature:** `def __init__(self, language: str = "python"):`

**Complete Code:**
```python
def __init__(self, language: str = "python"):
        if language not in self.LANGUAGES:
            raise ValueError(f"Unsupported: {language}")
        lang_module = self.LANGUAGES[language]
        self.language = Language(lang_module.language())
        self.parser = Parser(self.language)
        self.lang_name = language
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 7

---
#### parse_code (line 31)

**Signature:** `def parse_code(self, code: str):`

**Complete Code:**
```python
def parse_code(self, code: str):
        tree = self.parser.parse(bytes(code, "utf8"))
        return tree
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 3

---
#### extract_functions (line 95)

**Signature:** `def extract_functions(self, tree, code: str = None):`

**Complete Code:**
```python
def extract_functions(self, tree, code: str = None):
        """Extracting functions with complete code content"""

        queries = {
            "python": """
                (function_definition
                    name: (identifier) @function.name
                ) @function.def
            """,
            "javascript": """
                (function_declaration
                    name: (identifier) @function.name
                ) @function.def
                (function_expression
                    name: (identifier) @function.name
                ) @function.def
            """,
            "typescript": """
                (function_declaration
                    name: (identifier) @function.name
                ) @function.def
                (method_definition
                    name: (property_identifier) @function.name
                ) @function.def
            """,
            "go": """
                (function_declaration
                    name: (identifier) @function.name
                ) @function.def
                (method_declaration
                    name: (field_identifier) @function.name
                ) @function.def
            """,
            "rust": """
                (function_item
                    name: (identifier) @function.name
                ) @function.def
            """,
        }

        # For functions without complete definitions (like arrow functions)
        simple_queries = {
            "javascript": """
                (arrow_function) @function.arrow
            """,
        }

        functions = []

        # Splitting code into lines if provided
        code_lines = code.split("\n") if code else None

        # Extracting complete function definitions
        try:
            query = Query(self.language, queries[self.lang_name])
            query_cursor = QueryCursor(query)
            captures = query_cursor.captures(tree.root_node)

            for capture_name, nodes in captures.items():
                if "function.def" in capture_name:
                    for node in nodes:
                        func_node = node  # This is the complete function definition
                        name_node = None

                        # Find the function name within the function definition
                        for child in func_node.children:
                            if (
                                self.lang_name == "python"
                                and child.type == "identifier"
                            ):
                                name_node = child
                                break
                            elif (
                                self.lang_name in ["javascript", "typescript"]
                                and child.type == "identifier"
                            ):
                                name_node = child
                                break
                            elif self.lang_name == "go" and child.type == "identifier":
                                name_node = child
                                break
                            elif (
                                self.lang_name == "rust" and child.type == "identifier"
                            ):
                                name_node = child
                                break

                        if name_node:
                            func_name = name_node.text.decode("utf8")
                            line_num = name_node.start_point[0] + 1

                            # Extracting complete function code
                            start_byte = func_node.start_byte
                            end_byte = func_node.end_byte
                            complete_code = code[start_byte:end_byte] if code else ""

                            full_line = ""
                            if code_lines and line_num <= len(code_lines):
                                full_line = code_lines[line_num - 1]

                            functions.append(
                                {
                                    "name": func_name,
                                    "line": line_num,
                                    "full_line": full_line,
                                    "signature": full_line.strip(),
                                    "complete_code": complete_code,
                                    "start_byte": start_byte,
                                    "end_byte": end_byte,
                                    "start_point": func_node.start_point,
                                    "end_point": func_node.end_point,
                                    "code_lines": len(complete_code.split("\n")),
                                }
                            )

        except Exception as e:
            print(
                f"Warning: Complete function extraction failed for {self.lang_name}: {e}"
            )

        # Extracting simple functions (arrow functions, etc.)
        try:
            if self.lang_name in simple_queries:
                simple_query = Query(self.language, simple_queries[self.lang_name])
                simple_cursor = QueryCursor(simple_query)
                simple_captures = simple_cursor.captures(tree.root_node)

                for capture_name, nodes in simple_captures.items():
                    if "function.arrow" in capture_name:
                        for node in nodes:
                            line_num = node.start_point[0] + 1
                            start_byte = node.start_byte
                            end_byte = node.end_byte
                            complete_code = code[start_byte:end_byte] if code else ""

                            full_line = ""
                            if code_lines and line_num <= len(code_lines):
                                full_line = code_lines[line_num - 1]

                            # Generate name for anonymous functions
                            func_name = f"arrow_function_at_line_{line_num}"

                            functions.append(
                                {
                                    "name": func_name,
                                    "line": line_num,
                                    "full_line": full_line,
                                    "signature": full_line.strip(),
                                    "complete_code": complete_code,
                                    "start_byte": start_byte,
                                    "end_byte": end_byte,
                                    "start_point": node.start_point,
                                    "end_point": node.end_point,
                                    "code_lines": len(complete_code.split("\n")),
                                }
                            )

        except Exception as e:
            print(
                f"Warning: Simple function extraction failed for {self.lang_name}: {e}"
            )

        return functions
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 163

---
#### extract_dependencies (line 324)

**Signature:** `def extract_dependencies(self, tree, code: str = None):`

**Complete Code:**
```python
def extract_dependencies(self, tree, code: str = None):
        """
        DEPRECATED: Use extract_semantic_analysis() for better dependency analysis.
        This method is kept for backward compatibility but semantic graphs provide more accurate results.

        Returns basic dependency analysis using regex patterns.
        For enhanced analysis with AST-accurate relationships, use extract_semantic_analysis().
        """
        if not code:
            return {
                "internal_calls": [],
                "external_imports": [],
                "function_dependencies": {},
            }

        functions = self.extract_functions(tree, code)
        imports = self.extract_imports(tree)

        # Simplified dependency extraction (less accurate than semantic graphs)
        internal_calls = []
        external_imports = set()

        for func in functions:
            func_code = func["complete_code"]
            func_name = func["name"]

            # Basic function call detection (regex-based)
            for other_func in functions:
                if other_func["name"] != func_name:
                    pattern = r"\b" + re.escape(other_func["name"]) + r"\s*\("
                    if re.search(pattern, func_code):
                        internal_calls.append(
                            {
                                "caller": func_name,
                                "callee": other_func["name"],
                                "line": func["line"],
                            }
                        )

            # Basic import usage detection (text-based)
            for imp in imports:
                imp_name = imp["module"]
                if imp_name in func_code:
                    external_imports.add(imp_name)

        return {
            "internal_calls": internal_calls,
            "external_imports": list(external_imports),
            "function_dependencies": {
                func["name"]: [
                    call["callee"]
                    for call in internal_calls
                    if call["caller"] == func["name"]
                ]
                for func in functions
            },
        }
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 57

---
#### extract_semantic_analysis (line 382)

**Signature:** `def extract_semantic_analysis(`

**Complete Code:**
```python
def extract_semantic_analysis(
        self, tree, source_code: str, file_path: str
    ) -> Dict[str, Any]:
        """
        Enhanced analysis using semantic graphs (combining our approach with friend's approach)
        """
        try:
            # Create semantic graph builder
            graph_builder = SemanticGraphBuilder(self.lang_name)

            # Build semantic graph
            semantic_graph = graph_builder.build_semantic_graph(
                tree, source_code.encode("utf-8"), file_path
            )

            # Get analysis from semantic graph
            semantic_analysis = graph_builder.to_analysis_dict()

            # Enhance with our existing detailed function extraction
            our_functions = self.extract_functions(tree, source_code)
            our_imports = self.extract_imports(tree)
            our_classes = self.extract_classes(tree)

            # Merge both approaches for comprehensive analysis
            enhanced_analysis = {
                # Our detailed function analysis (with complete code)
                "detailed_functions": our_functions,
                "detailed_imports": our_imports,
                "detailed_classes": our_classes,
                # Friend's semantic graph analysis (relationships)
                "semantic_functions": semantic_analysis["functions"],
                "semantic_imports": semantic_analysis["imports"],
                "semantic_classes": semantic_analysis["classes"],
                "function_dependencies": semantic_analysis["dependencies"],
                "import_usage": semantic_analysis["import_usage"],
                # Graph statistics
                "graph_stats": semantic_analysis["graph_stats"],
                # Metadata
                "file_path": file_path,
                "language": self.lang_name,
                "analysis_method": "hybrid_detailed_semantic",
            }

            return enhanced_analysis

        except Exception as e:
            print(
                f"Warning: Semantic analysis failed, falling back to basic extraction: {e}"
            )
            # Fallback to our existing methods
            return {
                "detailed_functions": self.extract_functions(tree, source_code),
                "detailed_imports": self.extract_imports(tree),
                "detailed_classes": self.extract_classes(tree),
                "function_dependencies": self.extract_dependencies(tree, source_code)[
                    "function_dependencies"
                ],
                "analysis_method": "fallback_basic",
            }
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 59

---
### All Imports

- **typing** (from import, line 1)
- **tree_sitter** (from import, line 9)
- **tree_sitter_rust** (direct import, line 7)


## Enhanced File Analysis: `backend/src/services/enhanced_context_builder.py`

### Analysis Summary
- **Analysis Method**: ast_only
- **Functions**: 4
- **Classes**: 1
- **Imports**: 1
- **Semantic Graph Nodes**: 0
- **Semantic Graph Edges**: 0

### Classes

- **EnhancedContextBuilder** (line 4)

### Functions with Complete Code & Semantic Analysis

#### __init__ (line 10)

**Signature:** `def __init__(self):`

**Complete Code:**
```python
def __init__(self):
        pass
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 2

---
#### _detect_language (line 55)

**Signature:** `def _detect_language(self, file_path: str) -> [str]:`

**Complete Code:**
```python
def _detect_language(self, file_path: str) -> [str]:
        """Detect programming language from file extension"""
        ext = Path(file_path).suffix.lower()
        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".go": "go",
            ".rs": "rust",
        }
        return language_map.get(ext)
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 11

---
#### create_ast_markdown_context (line 13)

**Signature:** `def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:`

**Complete Code:**
```python
def create_ast_markdown_context(self, file_path: str, repo_path: str) -> str:
        """
        Creating markdown context from AST parser for a specific file
        """
        language = self._detect_language(file_path)
        if not language:
            return ""

        try:
            # Initializing AST parser for this language
            ast_parser = MultiLanguageAnalyzer(languag)

            # Read and analyze the file
            full_path = Path(repo_path) / file_path
            if not full_path.exists():
                return f"File not found: {file_path}"

            with open(full_path, "r", encoding="utf-8") as f:
                code = f.read()

            # Parsing and extracting information
            tree = ast_parser.parse_code(code)

            # Use enhanced semantic analysis (combining both approaches)
            semantic_analysis = ast_parser.extract_semantic_analysis(
                tree, code, file_path
            )

            # For backward compatibility, extract individual components
            functions = semantic_analysis.get("detailed_functions", [])
            imports = semantic_analysis.get("detailed_imports", [])
            dependencies = semantic_analysis.get("function_dependencies", {})
            classes = semantic_analysis.get("detailed_classes", [])

            # Convert to enhanced markdown with semantic information
            markdown = self._convert_to_enhanced_markdown(file_path, semantic_analysis)

            return markdown

        except Exception as e:
            return f"AST analysis failed for {file_path}: {str(e)}"
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 41

---
#### _convert_to_enhanced_markdown (line 67)

**Signature:** `def _convert_to_enhanced_markdown(`

**Complete Code:**
```python
def _convert_to_enhanced_markdown(
        self, file_path: str, semantic_analysis: Dict
    ) -> str:
        """Convert enhanced semantic analysis to markdown format with graph information"""

        # Extract components from semantic analysis
        detailed_functions = semantic_analysis.get("detailed_functions", [])
        detailed_imports = semantic_analysis.get("detailed_imports", [])
        detailed_classes = semantic_analysis.get("detailed_classes", [])
        function_dependencies = semantic_analysis.get("function_dependencies", {})
        import_usage = semantic_analysis.get("import_usage", {})
        graph_stats = semantic_analysis.get("graph_stats", {})
        analysis_method = semantic_analysis.get("analysis_method", "unknown")
```

**Semantic Analysis:**
- **Function Calls**: None
- **Imports Used**: None
- **Code Lines**: 13

---
### All Imports

- **datetime** (from import, line 1)




## AI Instructions

Please provide a comprehensive code review that considers:

1. **Code Quality**: Best practices, patterns, potential improvements
2. **Security**: Any security vulnerabilities or concerns
3. **Performance**: Performance implications and optimizations
4. **Dependencies**: Impact of new imports and function dependencies
5. **Maintainability**: Code readability and future maintenance
6. **Testing**: Testability and testing suggestions

Focus on the changed functions and their dependencies. Consider the PR history to avoid repeating previous suggestions.

---
*Context generated by enhanced AST parser + PR history analysis*
